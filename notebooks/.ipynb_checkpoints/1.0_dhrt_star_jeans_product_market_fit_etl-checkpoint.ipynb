{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6234a0e4",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#  IMPORTS & SETTINGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "d4e040f2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-30T13:09:14.115713Z",
     "start_time": "2023-01-30T13:09:12.562629Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import re\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "from sqlalchemy   import create_engine\n",
    "from datetime     import datetime\n",
    "from bs4          import BeautifulSoup\n",
    "from IPython.core.display     import HTML\n",
    "from matplotlib import pyplot as plt "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6811d089",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "50efc0c2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-30T10:55:02.551180Z",
     "start_time": "2023-01-30T10:55:01.644661Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def jupyter_settings():\n",
    "    %matplotlib inline\n",
    "    %pylab inline   \n",
    "    plt.style.use( 'bmh' )\n",
    "    plt.rcParams['figure.figsize'] = [20, 10]\n",
    "    plt.rcParams['font.size'] = 20\n",
    "    display( HTML( '<style>.container { width:100% !important; }</style>') )\n",
    "    pd.options.display.max_columns = None\n",
    "    pd.options.display.max_rows = None\n",
    "    pd.set_option( 'display.expand_frame_repr', False )\n",
    "    sns.set()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "740f3442",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-30T10:55:03.674758Z",
     "start_time": "2023-01-30T10:55:02.558716Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "%pylab is deprecated, use %matplotlib inline and import the required libraries.\n",
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/daniel/miniconda3/envs/ds_ao_dev/lib/python3.11/site-packages/IPython/core/magics/pylab.py:162: UserWarning: pylab import has clobbered these variables: ['datetime']\n",
      "`%matplotlib` prevents importing * from pylab and numpy\n",
      "  warn(\"pylab import has clobbered these variables: %s\"  % clobbered +\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "jupyter_settings()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c43af586",
   "metadata": {
    "hidden": true
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "883f200e",
   "metadata": {},
   "source": [
    "# EXTRACT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "541ed4df",
   "metadata": {},
   "source": [
    "The following features will be extracted of each product at showcase:\n",
    "    \n",
    "- product id\n",
    "- product_category\n",
    "- composition\n",
    "- fit\n",
    "- size\n",
    "- product_name\n",
    "- product_price\n",
    "- color_name\n",
    "- pieces/pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fa5c7e3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-26T18:10:46.685776Z",
     "start_time": "2023-01-26T18:10:46.683517Z"
    },
    "heading_collapsed": true
   },
   "source": [
    "## First Showcase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8b5b636f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-30T10:55:07.562709Z",
     "start_time": "2023-01-30T10:55:06.051523Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# API Request - GET\n",
    "url = 'https://www2.hm.com/en_us/men/products/jeans.html'\n",
    "\n",
    "header = {'User-agent':'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/109.0.0.0 Safari/537.36'}\n",
    "\n",
    "page = requests.get(url, headers = header)\n",
    "\n",
    "# page.text #doctype HTML\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d93badb8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-30T10:55:09.983724Z",
     "start_time": "2023-01-30T10:55:09.796057Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Beautiful Soup object\n",
    "soup = BeautifulSoup(page.text, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c7d3f570",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-30T10:55:14.185145Z",
     "start_time": "2023-01-30T10:55:13.843510Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Product Data\n",
    "products = soup.find('ul', class_ = 'products-listing small')\n",
    "product_list = products.find_all('article', class_ = \"hm-product-item\") #por que fiz retornar uma lista, e nao um elemento?\n",
    "\n",
    "# product_id\n",
    "product_id = [p.get('data-articlecode') for p in product_list]\n",
    "\n",
    "# product_category\n",
    "product_category = [p.get('data-category') for p in product_list]\n",
    "\n",
    "# product_name\n",
    "products_list_aux = products.find_all('a', class_ = 'link')\n",
    "product_name = [p.get_text('title') for p in products_list_aux]\n",
    "\n",
    "# product_price\n",
    "product_list = products.find_all('span', class_ = 'price regular')\n",
    "product_price = [p.get_text()for p in product_list]\n",
    "\n",
    "\n",
    "# Product Data to dataframe\n",
    "data = pd.DataFrame([product_id, product_category, product_name, product_price]).T\n",
    "data.columns = ['product_id','product_category','product_name','product_price']\n",
    "\n",
    "## scrapy_datetime\n",
    "data['scrapy_datetime'] = datetime.datetime.now().strftime( '%Y-%m-%d %H:%M:%S' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "8b343dc1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-27T18:28:24.085461Z",
     "start_time": "2023-01-27T18:28:24.074962Z"
    },
    "hidden": true,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product_id</th>\n",
       "      <th>product_category</th>\n",
       "      <th>product_name</th>\n",
       "      <th>product_price</th>\n",
       "      <th>scrapy_datetime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1024256006</td>\n",
       "      <td>men_jeans_slim</td>\n",
       "      <td>Slim Jeans</td>\n",
       "      <td>$ 24.99</td>\n",
       "      <td>2023-01-27 15:28:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0985159001</td>\n",
       "      <td>men_jeans_skinny</td>\n",
       "      <td>Skinny Jeans</td>\n",
       "      <td>$ 24.99</td>\n",
       "      <td>2023-01-27 15:28:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1130139004</td>\n",
       "      <td>men_jeans_loose</td>\n",
       "      <td>Loose Bootcut Jeans</td>\n",
       "      <td>$ 44.99</td>\n",
       "      <td>2023-01-27 15:28:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1008549006</td>\n",
       "      <td>men_jeans_regular</td>\n",
       "      <td>Regular Jeans</td>\n",
       "      <td>$ 24.99</td>\n",
       "      <td>2023-01-27 15:28:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0985159004</td>\n",
       "      <td>men_jeans_skinny</td>\n",
       "      <td>Skinny Jeans</td>\n",
       "      <td>$ 24.99</td>\n",
       "      <td>2023-01-27 15:28:22</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   product_id   product_category         product_name product_price      scrapy_datetime\n",
       "0  1024256006     men_jeans_slim           Slim Jeans       $ 24.99  2023-01-27 15:28:22\n",
       "1  0985159001   men_jeans_skinny         Skinny Jeans       $ 24.99  2023-01-27 15:28:22\n",
       "2  1130139004    men_jeans_loose  Loose Bootcut Jeans       $ 44.99  2023-01-27 15:28:22\n",
       "3  1008549006  men_jeans_regular        Regular Jeans       $ 24.99  2023-01-27 15:28:22\n",
       "4  0985159004   men_jeans_skinny         Skinny Jeans       $ 24.99  2023-01-27 15:28:22"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "96835eee",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-27T18:28:27.486270Z",
     "start_time": "2023-01-27T18:28:27.482359Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(36, 5)"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca668885",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-26T17:06:31.365692Z",
     "start_time": "2023-01-26T17:06:31.358444Z"
    },
    "hidden": true
   },
   "source": [
    "Ate aqui, foram extraidos 5 atributos dos 36 itens dessa primeira vitrine (showcase)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cbfcab4",
   "metadata": {},
   "source": [
    "## Each product of showcase"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69bc4daf",
   "metadata": {},
   "source": [
    "Ainda nessa pagina, alguns atributos nao ficam expostos na vitrine. Assim, é necessario acessar cada item, e daí extrair esses atributos: color and composition "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "15bc5c35",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-30T11:09:22.720258Z",
     "start_time": "2023-01-30T11:04:32.312739Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Product:https://www2.hm.com/en_us/men/products/jeans.html\n",
      "Product:https://www2.hm.com/en_us/productpage.1024256008.html\n",
      "Product:https://www2.hm.com/en_us/productpage.1024256008.html\n",
      "Product:https://www2.hm.com/en_us/productpage.0985159008.html\n",
      "Product:https://www2.hm.com/en_us/productpage.1130139003.html\n",
      "Product:https://www2.hm.com/en_us/productpage.1008549008.html\n",
      "Product:https://www2.hm.com/en_us/productpage.1119482005.html\n",
      "Product:https://www2.hm.com/en_us/productpage.0979945031.html\n",
      "Product:https://www2.hm.com/en_us/productpage.1130139004.html\n",
      "Product:https://www2.hm.com/en_us/productpage.0985159008.html\n",
      "Product:https://www2.hm.com/en_us/productpage.0979945031.html\n",
      "Product:https://www2.hm.com/en_us/productpage.1024256008.html\n",
      "Product:https://www2.hm.com/en_us/productpage.1008549008.html\n",
      "Product:https://www2.hm.com/en_us/productpage.0690449067.html\n",
      "Product:https://www2.hm.com/en_us/productpage.1024256008.html\n",
      "Product:https://www2.hm.com/en_us/productpage.0938875014.html\n",
      "Product:https://www2.hm.com/en_us/productpage.0690449067.html\n",
      "Product:https://www2.hm.com/en_us/productpage.0979945031.html\n",
      "Product:https://www2.hm.com/en_us/productpage.1119482003.html\n",
      "Product:https://www2.hm.com/en_us/productpage.0690449067.html\n",
      "Product:https://www2.hm.com/en_us/productpage.0985159007.html\n",
      "Product:https://www2.hm.com/en_us/productpage.1130139004.html\n",
      "Product:https://www2.hm.com/en_us/productpage.1024256008.html\n",
      "Product:https://www2.hm.com/en_us/productpage.1008110019.html\n",
      "Product:https://www2.hm.com/en_us/productpage.0979945031.html\n",
      "Product:https://www2.hm.com/en_us/productpage.1008110019.html\n",
      "Product:https://www2.hm.com/en_us/productpage.0979945031.html\n",
      "Product:https://www2.hm.com/en_us/productpage.0690449067.html\n",
      "Product:https://www2.hm.com/en_us/productpage.1024256008.html\n",
      "Product:https://www2.hm.com/en_us/productpage.1130309001.html\n",
      "Product:https://www2.hm.com/en_us/productpage.1008549008.html\n",
      "Product:https://www2.hm.com/en_us/productpage.1130139004.html\n",
      "Product:https://www2.hm.com/en_us/productpage.0985159008.html\n",
      "Product:https://www2.hm.com/en_us/productpage.1024711014.html\n",
      "Product:https://www2.hm.com/en_us/productpage.0690449064.html\n",
      "Product:https://www2.hm.com/en_us/productpage.1008110019.html\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product_id</th>\n",
       "      <th>composition</th>\n",
       "      <th>fit</th>\n",
       "      <th>size</th>\n",
       "      <th>product_name</th>\n",
       "      <th>product_price</th>\n",
       "      <th>color_name</th>\n",
       "      <th>style_ID</th>\n",
       "      <th>color_ID</th>\n",
       "      <th>scrapy_datetime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1024256006</td>\n",
       "      <td>Cotton 99%, Spandex 1%</td>\n",
       "      <td>Slim fit</td>\n",
       "      <td>The model is 187cm/6'2\" and wears a size 31/32</td>\n",
       "      <td>Slim Jeans</td>\n",
       "      <td>24.99</td>\n",
       "      <td>Dark denim blue</td>\n",
       "      <td>1024256</td>\n",
       "      <td>006</td>\n",
       "      <td>2023-01-30 08:09:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1024256006</td>\n",
       "      <td>Cotton 100%</td>\n",
       "      <td>Slim fit</td>\n",
       "      <td>The model is 187cm/6'2\" and wears a size 31/32</td>\n",
       "      <td>Slim Jeans</td>\n",
       "      <td>24.99</td>\n",
       "      <td>Dark denim blue</td>\n",
       "      <td>1024256</td>\n",
       "      <td>006</td>\n",
       "      <td>2023-01-30 08:09:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1024256001</td>\n",
       "      <td>Cotton 99%, Spandex 1%</td>\n",
       "      <td>Slim fit</td>\n",
       "      <td>The model is 180cm/5'11\" and wears a size 33/32</td>\n",
       "      <td>Slim Jeans</td>\n",
       "      <td>24.99</td>\n",
       "      <td>Black</td>\n",
       "      <td>1024256</td>\n",
       "      <td>001</td>\n",
       "      <td>2023-01-30 08:09:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1024256001</td>\n",
       "      <td>Polyester 65%, Cotton 35%</td>\n",
       "      <td>Slim fit</td>\n",
       "      <td>The model is 180cm/5'11\" and wears a size 33/32</td>\n",
       "      <td>Slim Jeans</td>\n",
       "      <td>24.99</td>\n",
       "      <td>Black</td>\n",
       "      <td>1024256</td>\n",
       "      <td>001</td>\n",
       "      <td>2023-01-30 08:09:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1024256002</td>\n",
       "      <td>Cotton 99%, Spandex 1%</td>\n",
       "      <td>Slim fit</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Slim Jeans</td>\n",
       "      <td>24.99</td>\n",
       "      <td>Light denim blue</td>\n",
       "      <td>1024256</td>\n",
       "      <td>002</td>\n",
       "      <td>2023-01-30 08:09:22</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   product_id                 composition       fit                                             size product_name product_price        color_name style_ID color_ID      scrapy_datetime\n",
       "0  1024256006      Cotton 99%, Spandex 1%  Slim fit   The model is 187cm/6'2\" and wears a size 31/32   Slim Jeans         24.99   Dark denim blue  1024256      006  2023-01-30 08:09:22\n",
       "1  1024256006                 Cotton 100%  Slim fit   The model is 187cm/6'2\" and wears a size 31/32   Slim Jeans         24.99   Dark denim blue  1024256      006  2023-01-30 08:09:22\n",
       "0  1024256001      Cotton 99%, Spandex 1%  Slim fit  The model is 180cm/5'11\" and wears a size 33/32   Slim Jeans         24.99             Black  1024256      001  2023-01-30 08:09:22\n",
       "1  1024256001   Polyester 65%, Cotton 35%  Slim fit  The model is 180cm/5'11\" and wears a size 33/32   Slim Jeans         24.99             Black  1024256      001  2023-01-30 08:09:22\n",
       "0  1024256002      Cotton 99%, Spandex 1%  Slim fit                                              NaN   Slim Jeans         24.99  Light denim blue  1024256      002  2023-01-30 08:09:22"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# empty DataFrame to receive all items \n",
    "df_all_items = pd.DataFrame()\n",
    "\n",
    "# aux dataframe to assert the correct scrap\n",
    "df_pattern = pd.DataFrame(columns=['Art. No.','Composition','Fit','Size', 'product_name','product_price'])\n",
    "\n",
    "# aux list to assert no different \n",
    "aux = []\n",
    "\n",
    "for i in range(len(data)): \n",
    "# for each item of showcase, enter at item and collect the details!\n",
    "    print('Product:{}'.format (url))\n",
    "    \n",
    "    # API Request\n",
    "    url = 'https://www2.hm.com/en_us/productpage.'+ data.loc[i,'product_id']+'.html'\n",
    "\n",
    "    header = {'User-agent':'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/109.0.0.0 Safari/537.36'}\n",
    "\n",
    "    page = requests.get(url,headers=header)\n",
    "\n",
    "    # page.text #doctypeHTML\n",
    "        \n",
    "        \n",
    "    # Beautiful Soup object\n",
    "    soup = BeautifulSoup(page.text, 'html.parser')\n",
    "    \n",
    "    \n",
    "    # --Product List\n",
    "    product_list = soup.find_all('a', class_ = \"filter-option miniature active\") + soup.find_all('a', class_ = \"filter-option miniature\") \n",
    "    # each miniature of color pants contains the tag with respective values, being one for active item and others tags for inactivate \n",
    "\n",
    "    # color_name\n",
    "    color_name = [p.get('data-color') for p in product_list]\n",
    "\n",
    "    # product_id    \n",
    "    product_id = [p.get('data-articlecode') for p in product_list]\n",
    "\n",
    "    # --Concat color_name and product_id to Dataframe\n",
    "    df_color = pd.DataFrame([product_id, color_name]).T\n",
    "    df_color.columns = ['product_id','color_name']\n",
    "\n",
    "   \n",
    "    for j in range(len(df_color)):\n",
    "\n",
    "        # API Request\n",
    "        url = 'https://www2.hm.com/en_us/productpage.'+ df_color.loc[j,'product_id']+'.html'\n",
    "        header = {'User-agent':'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/109.0.0.0 Safari/537.36'}\n",
    "#         print('Color:{}'.format (url))\n",
    "\n",
    "        page = requests.get(url, headers = header)\n",
    "\n",
    "        # Beautiful Soup object\n",
    "        soup = BeautifulSoup(page.text,'html.parser')\n",
    "    \n",
    "        # --product_name\n",
    "        product_name = soup.find_all('h1')[0]\n",
    "        product_name = product_name.get_text()\n",
    "        \n",
    "        # --product_price\n",
    "        product_price = soup.find_all('div', class_= \"primary-row product-item-price\")[0].get_text()\n",
    "        product_price = re.findall(r'\\d+\\.?\\d+', product_price)[0]\n",
    "        \n",
    "        # --product_details (size_model, fit, composition, art.no)\n",
    "        product_details_list = soup.find_all('div', class_ = \"content pdp-text pdp-content\")[0].find('dl').find_all('div')\n",
    "        product_details = [list(filter(None,p.get_text().split('\\n'))) for p in product_details_list]\n",
    "\n",
    "        # product_details to DataFrame\n",
    "        df_details = pd.DataFrame(product_details).T\n",
    "        df_details.columns = df_details.iloc[0]\n",
    "        \n",
    "        # delete first row of df_details\n",
    "        df_details = df_details.iloc[1:]\n",
    "        \n",
    "        # fillna in Size, Fit and Art.No with the same value,\n",
    "        df_details = df_details.fillna(method='ffill')\n",
    "        \n",
    "        # remove Shell:, Pocket Lining:, Lining:, Pocket:\n",
    "        df_details['Composition'] = df_details['Composition'].str.replace('Shell:', '', regex=True)\n",
    "        df_details['Composition'] = df_details['Composition'].str.replace('Pocket lining:', '', regex=True)\n",
    "        df_details['Composition'] = df_details['Composition'].str.replace('Lining:', '', regex=True)\n",
    "        df_details['Composition'] = df_details['Composition'].str.replace('Pocket:', '', regex=True)\n",
    "        # //the percentage of components was abstracted. To find the components apply df_details['Composition'].unique on df done\n",
    "        \n",
    "        # add product_price and product_name to df_details\n",
    "        df_details['product_price'] = product_price\n",
    "        df_details['product_name'] = product_name\n",
    "        \n",
    "        \n",
    "        # garantee the same columns between product_details and a pattern, and sort the labels\n",
    "        df_details = pd.concat([df_pattern,df_details], axis=0)\n",
    "        # //if it has some difference, a new column will appear with some Nan\n",
    "        \n",
    "        # rename the columns to lower case\n",
    "        df_details.columns = df_details.columns.map(str.lower)\n",
    "        df_details = df_details.rename(columns={'art. no.':'product_id'})\n",
    "#         df_details.columns = ['']\n",
    "\n",
    "        \n",
    "        # if some strange column appear, keep it and shows\n",
    "        aux = aux + df_details.columns.to_list()\n",
    "        if len(set(aux)) != len(df_pattern.columns):\n",
    "            print('Some column does not fit with pattern!')\n",
    "            pass\n",
    "                \n",
    "        # merge df_color and df_details of one item (single product_id)\n",
    "        df_details = pd.merge(df_details, df_color, how='left', on='product_id')\n",
    "        \n",
    "        # add to list of all items\n",
    "        df_all_items = pd.concat([df_all_items, df_details])\n",
    "        \n",
    "\n",
    "# generate style_ID + color_ID\n",
    "df_all_items ['style_ID'] = df_all_items['product_id'].apply(lambda x: x[:-3])\n",
    "df_all_items ['color_ID'] = df_all_items['product_id'].apply(lambda x: x[-3:])\n",
    "\n",
    "## scrapy_datetime\n",
    "df_all_items['scrapy_datetime'] = datetime.datetime.now().strftime( '%Y-%m-%d %H:%M:%S' )\n",
    "\n",
    "df_all_items.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "360e8746",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# TRANSFORM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5964cada",
   "metadata": {
    "hidden": true
   },
   "source": [
    "At this step, I will clean the data. The following tasks will be done:\n",
    "\n",
    " - Split the compositions and their rate of materials\n",
    " - Join the same product_id \n",
    " - snake_case all items\n",
    " - size column mantain only the size of model in cm\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "8fd4a443",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-30T12:18:17.829069Z",
     "start_time": "2023-01-30T12:18:17.824838Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df1 = df_all_items.copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "7b722bbf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-30T12:25:31.258264Z",
     "start_time": "2023-01-30T12:25:31.188976Z"
    },
    "code_folding": [],
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# product_id\n",
    "\n",
    "# product_name\n",
    "df1['product_name'] = df1.loc[:,'product_name'].apply(lambda x: x.replace(' ','_').lower())\n",
    "\n",
    "#product_fit\n",
    "df1['fit'] = df1.loc[:,'fit'].apply(lambda x: x.replace(' ','_').lower())\n",
    "\n",
    "#color_name\n",
    "df1['color_name'] = df1.loc[:,'color_name'].apply(lambda x: x.replace(' ','_').lower())\n",
    "\n",
    "#size_number\n",
    "df1['size_number'] = df1['size'].apply(lambda x: re.search('\\d{3}cm', x).group(0) if pd.notnull(x) else x)\n",
    "df1['size_number'] = df1['size_number'].apply(lambda x: re.search('\\d+', x).group(0) if pd.notnull(x) else x)\n",
    "\n",
    "#size_model \n",
    "df1['size_model'] = df1['size'].str.extract('(\\d+/\\d+)')\n",
    "\n",
    "\n",
    "\n",
    "# df1_composition outuput of break in columns of each item of 'composition' separated by a comma\n",
    "df1_composition = df1['composition'].str.split(',', expand = True).reset_index(drop=True)\n",
    "\n",
    "# df_aux to storage a kind of composition in a separated column \n",
    "df_aux = pd.DataFrame(index = np.arange(len(df1)), columns = ['cotton','spandex','polyester','elastomultiester'])\n",
    "\n",
    "\n",
    "\n",
    "# --composition: cotton\n",
    "df1_cotton_0 = df1_composition.loc[df1_composition[0].str.contains('Cotton', na=True), 0]\n",
    "df1_cotton_0.name = 'cotton'\n",
    "df1_cotton_1 = df1_composition.loc[df1_composition[1].str.contains('Cotton', na=True),1]\n",
    "df1_cotton_1.name = 'cotton'\n",
    "\n",
    "df1_cotton = df1_cotton_0.combine_first(df1_cotton_1)\n",
    "\n",
    "df_aux = pd.concat ([df_aux, df1_cotton],axis=1)\n",
    "df_aux = df_aux.iloc[:, ~df_aux.columns.duplicated(keep='last')]\n",
    "\n",
    "# --composition: spandex\n",
    "\n",
    "df1_spandex_0 = df1_composition.loc[df1_composition[1].str.contains('Spandex', na=True),1]\n",
    "df1_spandex_0.name = 'spandex'\n",
    "df1_spandex_1 = df1_composition.loc[df1_composition[2].str.contains('Spandex', na=True),2]\n",
    "df1_spandex_1.name = 'spandex'\n",
    "\n",
    "df1_spandex = df1_spandex_0.combine_first(df1_spandex_1)\n",
    "\n",
    "df_aux = pd.concat([df_aux, df1_spandex], axis=1)\n",
    "df_aux = df_aux.iloc[:, ~df_aux.columns.duplicated(keep='last')]\n",
    "\n",
    "# --composition: polyester\n",
    "\n",
    "df1_polyester = df1_composition.loc[df1_composition[0].str.contains('Polyester', na=True),0]\n",
    "df1_polyester.name = 'polyester'\n",
    "\n",
    "df_aux = pd.concat([df_aux, df1_polyester], axis=1)\n",
    "df_aux = df_aux.iloc[:, ~df_aux.columns.duplicated(keep='last')]\n",
    "\n",
    "\n",
    "# --composition: elastomultiester\n",
    "\n",
    "df1_elastomultiester = df1_composition.loc[df1_composition[1].str.contains('Elastomultiester', na=True),1]\n",
    "df1_elastomultiester.name = 'elastomultiester'\n",
    "\n",
    "df_aux = pd.concat([df_aux, df1_elastomultiester], axis=1)\n",
    "df_aux = df_aux.iloc[:, ~df_aux.columns.duplicated(keep='last')]\n",
    "\n",
    "# add product_id to df_aux\n",
    "df_aux = pd.concat([df1.loc[:,'product_id'].reset_index(drop=True),df_aux], axis=1)\n",
    "\n",
    "\n",
    "#format composition \n",
    "df_aux['cotton'] = df_aux['cotton'].apply(lambda x: int(re.search('\\d+',x).group(0)) /100 if pd.notnull(x) else x)\n",
    "df_aux['polyester'] = df_aux['polyester'].apply(lambda x: int(re.search('\\d+',x).group(0))/100 if pd.notnull(x) else x)\n",
    "df_aux['spandex'] = df_aux['spandex'].apply(lambda x: int(re.search('\\d+',x).group(0))/100 if pd.notnull(x) else x)\n",
    "df_aux['elastomultiester'] = df_aux['elastomultiester'].apply(lambda x: int(re.search('\\d+',x).group(0))/100 if pd.notnull(x) else x)\n",
    "\n",
    "#final join\n",
    "df_aux = df_aux.groupby('product_id').max().reset_index().fillna(0)\n",
    "df1 = pd.merge(df1, df_aux, on = 'product_id', how = 'left')\n",
    "\n",
    "\n",
    "# drop columns \n",
    "df1 = df1.drop(columns = ['size', 'composition'])\n",
    "\n",
    "# drop duplicates\n",
    "df1 = df1.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "cc43a297",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-30T12:36:28.381957Z",
     "start_time": "2023-01-30T12:36:28.377647Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['product_id', 'fit', 'product_name', 'product_price', 'color_name',\n",
       "       'style_ID', 'color_ID', 'scrapy_datetime', 'size_number', 'size_model',\n",
       "       'cotton', 'spandex', 'polyester', 'elastomultiester'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84890f25",
   "metadata": {},
   "source": [
    "# LOAD DATA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1c59fba",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-30T17:22:19.149066Z",
     "start_time": "2023-01-30T17:22:19.146776Z"
    }
   },
   "source": [
    "## Create Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "af04e9b1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-30T17:21:58.441426Z",
     "start_time": "2023-01-30T17:21:58.438856Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create Schema\n",
    "query_showroom_schema = \"\"\"\n",
    "    CREATE TABLE vitrine(\n",
    "    product_id           TEXT,\n",
    "    fit                  TEXT,\n",
    "    product_name         TEXT,\n",
    "    product_price        TEXT,\n",
    "    color_name           TEXT,\n",
    "    style_ID             TEXT,\n",
    "    color_ID             TEXT,\n",
    "    scrapy_datetime      TEXT,\n",
    "    size_number          TEXT,\n",
    "    size_model           TEXT,\n",
    "    cotton               REAL,\n",
    "    spandex              REAL,\n",
    "    polyester            REAL,\n",
    "    elastomultiester     REAL\n",
    "    )\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "7d4937ff",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-30T17:22:00.649603Z",
     "start_time": "2023-01-30T17:21:59.133005Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create table\n",
    "path = '../data/raw/'\n",
    "conn = sqlite3.connect(path + 'db_hm.sqlite')\n",
    "cursor = conn.execute(query_showroom_schema)\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "c63578b5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-30T17:22:02.968813Z",
     "start_time": "2023-01-30T17:22:02.961889Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>vitrine</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      name\n",
       "0  vitrine"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test if table was created at database\n",
    "conn = sqlite3.connect(path + 'db_hm.sqlite')\n",
    "\n",
    "query = \"\"\"\n",
    "    SELECT name\n",
    "    FROM   sqlite_master\n",
    "    WHERE  type = 'table'\n",
    "\"\"\"\n",
    "\n",
    "table = pd.read_sql_query(query, conn)\n",
    "table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad395aca",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-30T17:22:32.769001Z",
     "start_time": "2023-01-30T17:22:32.766518Z"
    }
   },
   "source": [
    "## Insert data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "bd6dc9d4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-30T17:22:47.356052Z",
     "start_time": "2023-01-30T17:22:46.744494Z"
    }
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['cotton', 'spandex', 'polyester', 'elastomultiester'] not in index\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[172], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Create data_insert from df1 with adjusts at orders of columns\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m data_insert \u001b[38;5;241m=\u001b[39m \u001b[43mdf1\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mproduct_id\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mstyle_ID\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcolor_ID\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mproduct_name\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcolor_name\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfit\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mproduct_price\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msize_number\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msize_model\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcotton\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mspandex\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpolyester\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43melastomultiester\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mscrapy_datetime\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m \u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/ds_ao_dev/lib/python3.11/site-packages/pandas/core/frame.py:3813\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3811\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n\u001b[1;32m   3812\u001b[0m         key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[0;32m-> 3813\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_indexer_strict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcolumns\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m   3815\u001b[0m \u001b[38;5;66;03m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[1;32m   3816\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(indexer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/envs/ds_ao_dev/lib/python3.11/site-packages/pandas/core/indexes/base.py:6070\u001b[0m, in \u001b[0;36mIndex._get_indexer_strict\u001b[0;34m(self, key, axis_name)\u001b[0m\n\u001b[1;32m   6067\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   6068\u001b[0m     keyarr, indexer, new_indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reindex_non_unique(keyarr)\n\u001b[0;32m-> 6070\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raise_if_missing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeyarr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   6072\u001b[0m keyarr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtake(indexer)\n\u001b[1;32m   6073\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[1;32m   6074\u001b[0m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/ds_ao_dev/lib/python3.11/site-packages/pandas/core/indexes/base.py:6133\u001b[0m, in \u001b[0;36mIndex._raise_if_missing\u001b[0;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[1;32m   6130\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   6132\u001b[0m not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask\u001b[38;5;241m.\u001b[39mnonzero()[\u001b[38;5;241m0\u001b[39m]]\u001b[38;5;241m.\u001b[39munique())\n\u001b[0;32m-> 6133\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not in index\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyError\u001b[0m: \"['cotton', 'spandex', 'polyester', 'elastomultiester'] not in index\""
     ]
    }
   ],
   "source": [
    "# Create data_insert from df1 with adjusts at orders of columns\n",
    "data_insert = df1[[\n",
    "    'product_id',\n",
    "    'style_ID', \n",
    "    'color_ID', \n",
    "    'product_name',\n",
    "    'color_name',\n",
    "    'fit',\n",
    "    'product_price', \n",
    "    'size_number', \n",
    "    'size_model',\n",
    "    'cotton', \n",
    "    'spandex', \n",
    "    'polyester', \n",
    "    'elastomultiester',\n",
    "    'scrapy_datetime' \n",
    "]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26d3a245",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-30T17:22:47.360669Z",
     "start_time": "2023-01-30T17:22:47.360649Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create database connection\n",
    "conn = create_engine('sqlite:///'+ path + 'db_hm.sqlite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "5c469a93",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-30T17:22:47.978523Z",
     "start_time": "2023-01-30T17:22:47.529034Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "91"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Data Insert\n",
    "data_insert.to_sql('vitrine', con = conn, if_exists = 'append', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "cc450406",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-30T17:22:49.148375Z",
     "start_time": "2023-01-30T17:22:49.131438Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product_id</th>\n",
       "      <th>fit</th>\n",
       "      <th>product_name</th>\n",
       "      <th>product_price</th>\n",
       "      <th>color_name</th>\n",
       "      <th>style_ID</th>\n",
       "      <th>color_ID</th>\n",
       "      <th>scrapy_datetime</th>\n",
       "      <th>size_number</th>\n",
       "      <th>size_model</th>\n",
       "      <th>cotton</th>\n",
       "      <th>spandex</th>\n",
       "      <th>polyester</th>\n",
       "      <th>elastomultiester</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1024256006</td>\n",
       "      <td>slim_fit</td>\n",
       "      <td>slim_jeans</td>\n",
       "      <td>24.99</td>\n",
       "      <td>dark_denim_blue</td>\n",
       "      <td>1024256</td>\n",
       "      <td>006</td>\n",
       "      <td>2023-01-30 08:09:22</td>\n",
       "      <td>187</td>\n",
       "      <td>31/32</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1024256001</td>\n",
       "      <td>slim_fit</td>\n",
       "      <td>slim_jeans</td>\n",
       "      <td>24.99</td>\n",
       "      <td>black</td>\n",
       "      <td>1024256</td>\n",
       "      <td>001</td>\n",
       "      <td>2023-01-30 08:09:22</td>\n",
       "      <td>180</td>\n",
       "      <td>33/32</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1024256002</td>\n",
       "      <td>slim_fit</td>\n",
       "      <td>slim_jeans</td>\n",
       "      <td>24.99</td>\n",
       "      <td>light_denim_blue</td>\n",
       "      <td>1024256</td>\n",
       "      <td>002</td>\n",
       "      <td>2023-01-30 08:09:22</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1024256003</td>\n",
       "      <td>slim_fit</td>\n",
       "      <td>slim_jeans</td>\n",
       "      <td>24.99</td>\n",
       "      <td>light_denim_blue</td>\n",
       "      <td>1024256</td>\n",
       "      <td>003</td>\n",
       "      <td>2023-01-30 08:09:22</td>\n",
       "      <td>180</td>\n",
       "      <td>33/32</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1024256004</td>\n",
       "      <td>slim_fit</td>\n",
       "      <td>slim_jeans</td>\n",
       "      <td>24.99</td>\n",
       "      <td>denim_blue</td>\n",
       "      <td>1024256</td>\n",
       "      <td>004</td>\n",
       "      <td>2023-01-30 08:09:22</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1024256005</td>\n",
       "      <td>slim_fit</td>\n",
       "      <td>slim_jeans</td>\n",
       "      <td>24.99</td>\n",
       "      <td>dark_blue</td>\n",
       "      <td>1024256</td>\n",
       "      <td>005</td>\n",
       "      <td>2023-01-30 08:09:22</td>\n",
       "      <td>183</td>\n",
       "      <td>31/32</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1024256007</td>\n",
       "      <td>slim_fit</td>\n",
       "      <td>slim_jeans</td>\n",
       "      <td>19.99</td>\n",
       "      <td>dark_gray</td>\n",
       "      <td>1024256</td>\n",
       "      <td>007</td>\n",
       "      <td>2023-01-30 08:09:22</td>\n",
       "      <td>187</td>\n",
       "      <td>31/32</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1024256008</td>\n",
       "      <td>slim_fit</td>\n",
       "      <td>slim_jeans</td>\n",
       "      <td>24.99</td>\n",
       "      <td>white</td>\n",
       "      <td>1024256</td>\n",
       "      <td>008</td>\n",
       "      <td>2023-01-30 08:09:22</td>\n",
       "      <td>183</td>\n",
       "      <td>32/32</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0985159001</td>\n",
       "      <td>skinny_fit</td>\n",
       "      <td>skinny_jeans</td>\n",
       "      <td>24.99</td>\n",
       "      <td>black</td>\n",
       "      <td>0985159</td>\n",
       "      <td>001</td>\n",
       "      <td>2023-01-30 08:09:22</td>\n",
       "      <td>180</td>\n",
       "      <td>33/32</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0985159002</td>\n",
       "      <td>skinny_fit</td>\n",
       "      <td>skinny_jeans</td>\n",
       "      <td>24.99</td>\n",
       "      <td>denim_blue</td>\n",
       "      <td>0985159</td>\n",
       "      <td>002</td>\n",
       "      <td>2023-01-30 08:09:22</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   product_id         fit  product_name product_price        color_name style_ID color_ID      scrapy_datetime size_number size_model  cotton  spandex  polyester  elastomultiester\n",
       "0  1024256006    slim_fit    slim_jeans         24.99   dark_denim_blue  1024256      006  2023-01-30 08:09:22         187      31/32    1.00     0.01       0.00               0.0\n",
       "1  1024256001    slim_fit    slim_jeans         24.99             black  1024256      001  2023-01-30 08:09:22         180      33/32    0.99     0.01       0.65               0.0\n",
       "2  1024256002    slim_fit    slim_jeans         24.99  light_denim_blue  1024256      002  2023-01-30 08:09:22        None       None    0.99     0.01       0.65               0.0\n",
       "3  1024256003    slim_fit    slim_jeans         24.99  light_denim_blue  1024256      003  2023-01-30 08:09:22         180      33/32    0.99     0.01       0.65               0.0\n",
       "4  1024256004    slim_fit    slim_jeans         24.99        denim_blue  1024256      004  2023-01-30 08:09:22        None       None    0.99     0.01       0.65               0.0\n",
       "5  1024256005    slim_fit    slim_jeans         24.99         dark_blue  1024256      005  2023-01-30 08:09:22         183      31/32    0.99     0.01       0.65               0.0\n",
       "6  1024256007    slim_fit    slim_jeans         19.99         dark_gray  1024256      007  2023-01-30 08:09:22         187      31/32    0.99     0.01       0.65               0.0\n",
       "7  1024256008    slim_fit    slim_jeans         24.99             white  1024256      008  2023-01-30 08:09:22         183      32/32    1.00     0.01       0.00               0.0\n",
       "8  0985159001  skinny_fit  skinny_jeans         24.99             black  0985159      001  2023-01-30 08:09:22         180      33/32    1.00     0.01       0.00               0.0\n",
       "9  0985159002  skinny_fit  skinny_jeans         24.99        denim_blue  0985159      002  2023-01-30 08:09:22        None       None    1.00     0.01       0.00               0.0"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test if vitrine was populated\n",
    "conn = sqlite3.connect(path + 'db_hm.sqlite')\n",
    "\n",
    "query = \"\"\"\n",
    "    SELECT *\n",
    "    FROM   vitrine\n",
    "    LIMIT 10\n",
    "\"\"\"\n",
    "\n",
    "table = pd.read_sql_query(query, conn)\n",
    "table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95806d66",
   "metadata": {},
   "source": [
    "# ETL TO PRODUCTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "4431e882",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-30T17:24:28.709726Z",
     "start_time": "2023-01-30T17:24:24.847891Z"
    },
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"<stdin>\", line 341, in <module>\n",
      "TypeError: get_all_product_details() missing 1 required positional argument: 'header'\n"
     ]
    },
    {
     "ename": "CalledProcessError",
     "evalue": "Command 'b'\\n# IMPORTS\\nimport requests\\nimport re\\nimport sqlite3\\nimport pandas as pd\\nimport seaborn as sns\\nimport logging\\nimport os\\n\\nfrom sqlalchemy   import create_engine\\nfrom datetime     import datetime\\nfrom bs4          import BeautifulSoup\\n\\n\\n# Data extract - first Showcase\\n\\ndef get_showcase(url, header):\\n    \\n    # API Request - GET\\n    page = requests.get(url, headers = header)\\n    \\n    # Beautiful Soup object\\n    soup = BeautifulSoup(page.text, \\'html.parser\\')\\n    \\n    # Product Data\\n    products = soup.find(\\'ul\\', class_ = \\'products-listing small\\')\\n    product_list = products.find_all(\\'article\\', class_ = \"hm-product-item\") \\n\\n    # product_id\\n    product_id = [p.get(\\'data-articlecode\\') for p in product_list]\\n\\n    # product_category\\n    product_category = [p.get(\\'data-category\\') for p in product_list]\\n\\n    # product_name\\n    products_list_aux = products.find_all(\\'a\\', class_ = \\'link\\')\\n    product_name = [p.get_text(\\'title\\') for p in products_list_aux]\\n\\n    # product_price\\n    product_list = products.find_all(\\'span\\', class_ = \\'price regular\\')\\n    product_price = [p.get_text()for p in product_list]\\n\\n\\n    # Product Data to dataframe\\n    df_showcase = pd.DataFrame([product_id, product_category, product_name, product_price]).T\\n    df_showcase.columns = [\\'product_id\\',\\'product_category\\',\\'product_name\\',\\'product_price\\']\\n\\n    ## scrapy_datetime\\n    df_showcase[\\'scrapy_datetime\\'] = datetime.now().strftime( \\'%Y-%m-%d %H:%M:%S\\' )\\n    \\n    return df_showcase\\n\\n\\n\\n# Data extract by product - Each product of this showcase\\n\\ndef get_all_product_details(data, header):\\n        \\n    # empty DataFrame to receive all items \\n    df_all_product_details = pd.DataFrame()\\n\\n    # aux dataframe to assert the correct scrap\\n    df_pattern = pd.DataFrame(columns=[\\'Art. No.\\',\\'Composition\\',\\'Fit\\',\\'Size\\', \\'product_name\\',\\'product_price\\'])\\n\\n    # aux list to assert no different \\n    aux = []\\n\\n    for i in range(len(data)): \\n    # for each item of showcase, enter at item and collect the details!\\n        logger.debug (\\'Product: %s\\',url)\\n\\n        # API Request\\n        url = \\'https://www2.hm.com/en_us/productpage.\\'+ data.loc[i,\\'product_id\\']+\\'.html\\'\\n\\n        header = {\\'User-agent\\':\\'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/109.0.0.0 Safari/537.36\\'}\\n\\n        page = requests.get(url,headers=header)\\n\\n        # page.text #doctypeHTML\\n\\n\\n        # Beautiful Soup object\\n        soup = BeautifulSoup(page.text, \\'html.parser\\')\\n\\n\\n        # --Product List\\n        product_list = soup.find_all(\\'a\\', class_ = \"filter-option miniature active\") + soup.find_all(\\'a\\', class_ = \"filter-option miniature\") \\n        # each miniature of color pants contains the tag with respective values, being one for active item and others tags for inactivate \\n\\n        # color_name\\n        color_name = [p.get(\\'data-color\\') for p in product_list]\\n\\n        # product_id    \\n        product_id = [p.get(\\'data-articlecode\\') for p in product_list]\\n\\n        # --Concat color_name and product_id to Dataframe\\n        df_color = pd.DataFrame([product_id, color_name]).T\\n        df_color.columns = [\\'product_id\\',\\'color_name\\']\\n\\n\\n        for j in range(len(df_color)):\\n\\n            # API Request\\n            url = \\'https://www2.hm.com/en_us/productpage.\\'+ df_color.loc[j,\\'product_id\\']+\\'.html\\'\\n            header = {\\'User-agent\\':\\'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/109.0.0.0 Safari/537.36\\'}\\n            logger.debug(\\'Color: %s\\',url)\\n\\n            page = requests.get(url, headers = header)\\n\\n            # Beautiful Soup object\\n            soup = BeautifulSoup(page.text,\\'html.parser\\')\\n\\n            # --product_name\\n            product_name = soup.find_all(\\'h1\\')[0]\\n            product_name = product_name.get_text()\\n\\n            # --product_price\\n            product_price = soup.find_all(\\'div\\', class_= \"primary-row product-item-price\")[0].get_text()\\n            product_price = re.findall(r\\'\\\\d+\\\\.?\\\\d+\\', product_price)[0]\\n\\n            # --product_details (size_model, fit, composition, art.no)\\n            product_details_list = soup.find_all(\\'div\\', class_ = \"content pdp-text pdp-content\")[0].find(\\'dl\\').find_all(\\'div\\')\\n            product_details = [list(filter(None,p.get_text().split(\\'\\\\n\\'))) for p in product_details_list]\\n\\n            # product_details to DataFrame\\n            df_details = pd.DataFrame(product_details).T\\n            df_details.columns = df_details.iloc[0]\\n\\n            # delete first row of df_details\\n            df_details = df_details.iloc[1:]\\n\\n            # fillna in Size, Fit and Art.No with the same value,\\n            df_details = df_details.fillna(method=\\'ffill\\')\\n\\n            # remove Shell:, Pocket Lining:, Lining:, Pocket:\\n            df_details[\\'Composition\\'] = df_details[\\'Composition\\'].str.replace(\\'Shell:\\', \\'\\', regex=True)\\n            df_details[\\'Composition\\'] = df_details[\\'Composition\\'].str.replace(\\'Pocket lining:\\', \\'\\', regex=True)\\n            df_details[\\'Composition\\'] = df_details[\\'Composition\\'].str.replace(\\'Lining:\\', \\'\\', regex=True)\\n            df_details[\\'Composition\\'] = df_details[\\'Composition\\'].str.replace(\\'Pocket:\\', \\'\\', regex=True)\\n            # //the percentage of components was abstracted. To find the components apply df_details[\\'Composition\\'].unique on df done\\n\\n            # add product_price and product_name to df_details\\n            df_details[\\'product_price\\'] = product_price\\n            df_details[\\'product_name\\'] = product_name\\n\\n\\n            # garantee the same columns between product_details and a pattern, and sort the labels\\n            df_details = pd.concat([df_pattern,df_details], axis=0)\\n            # //if it has some difference, a new column will appear with some Nan\\n\\n            # rename the columns to lower case\\n            df_details.columns = df_details.columns.map(str.lower)\\n            df_details = df_details.rename(columns={\\'art. no.\\':\\'product_id\\'})\\n    #         df_details.columns = [\\'\\']\\n\\n\\n            # if some strange column appear, keep it and shows\\n            aux = aux + df_details.columns.to_list()\\n            if len(set(aux)) != len(df_pattern.columns):\\n                print(\\'Some column does not fit with pattern!\\')\\n                pass\\n\\n            # merge df_color and df_details of one item (single product_id)\\n            df_details = pd.merge(df_details, df_color, how=\\'left\\', on=\\'product_id\\')\\n\\n            # add to list of all items\\n            df_all_product_details = pd.concat([df_all_product_details, df_details])\\n\\n\\n    # generate style_ID + color_ID\\n    df_all_product_details [\\'style_ID\\'] = df_all_product_details[\\'product_id\\'].apply(lambda x: x[:-3])\\n    df_all_product_details [\\'color_ID\\'] = df_all_product_details[\\'product_id\\'].apply(lambda x: x[-3:])\\n\\n    ## scrapy_datetime\\n    df_all_product_details[\\'scrapy_datetime\\'] = datetime.now().strftime( \\'%Y-%m-%d %H:%M:%S\\' )\\n\\n    return df_all_product_details\\n\\n\\n\\n# Data transform - cleaning the data\\n\\ndef data_cleaning (data):\\n\\n    # product_id\\n\\n    # product_name\\n    data[\\'product_name\\'] = data.loc[:,\\'product_name\\'].apply(lambda x: x.replace(\\' \\',\\'_\\').lower())\\n\\n    #product_fit\\n    data[\\'fit\\'] = data.loc[:,\\'fit\\'].apply(lambda x: x.replace(\\' \\',\\'_\\').lower())\\n\\n    #color_name\\n    data[\\'color_name\\'] = data.loc[:,\\'color_name\\'].apply(lambda x: x.replace(\\' \\',\\'_\\').lower())\\n\\n    #size_number\\n    data[\\'size_number\\'] = data[\\'size\\'].apply(lambda x: re.search(\\'\\\\d{3}cm\\', x).group(0) if pd.notnull(x) else x)\\n    data[\\'size_number\\'] = data[\\'size_number\\'].apply(lambda x: re.search(\\'\\\\d+\\', x).group(0) if pd.notnull(x) else x)\\n\\n    #size_model \\n    data[\\'size_model\\'] = data[\\'size\\'].str.extract(\\'(\\\\d+/\\\\d+)\\')\\n\\n\\n\\n    # df1_composition outuput of break in columns of each item of \\'composition\\' separated by a comma\\n    df_composition = data[\\'composition\\'].str.split(\\',\\', expand = True).reset_index(drop=True)\\n\\n    # df_aux to storage a kind of composition in a separated column \\n    df_aux = pd.DataFrame(index = np.arange(len(df1)), columns = [\\'cotton\\',\\'spandex\\',\\'polyester\\',\\'elastomultiester\\'])\\n\\n\\n\\n    # --composition: cotton\\n    df_cotton_0 = df_composition.loc[df_composition[0].str.contains(\\'Cotton\\', na=True), 0]\\n    df_cotton_0.name = \\'cotton\\'\\n    df_cotton_1 = df_composition.loc[df_composition[1].str.contains(\\'Cotton\\', na=True),1]\\n    df_cotton_1.name = \\'cotton\\'\\n\\n    df_cotton = df_cotton_0.combine_first(df_cotton_1)\\n\\n    df_aux = pd.concat ([df_aux, df_cotton],axis=1)\\n    df_aux = df_aux.iloc[:, ~df_aux.columns.duplicated(keep=\\'last\\')]\\n\\n    # --composition: spandex\\n\\n    df_spandex_0 = df_composition.loc[df_composition[1].str.contains(\\'Spandex\\', na=True),1]\\n    df_spandex_0.name = \\'spandex\\'\\n    df_spandex_1 = df_composition.loc[df_composition[2].str.contains(\\'Spandex\\', na=True),2]\\n    df_spandex_1.name = \\'spandex\\'\\n\\n    df_spandex = df1_spandex_0.combine_first(df1_spandex_1)\\n\\n    df_aux = pd.concat([df_aux, df_spandex], axis=1)\\n    df_aux = df_aux.iloc[:, ~df_aux.columns.duplicated(keep=\\'last\\')]\\n\\n    # --composition: polyester\\n\\n    df_polyester = df_composition.loc[df_composition[0].str.contains(\\'Polyester\\', na=True),0]\\n    df_polyester.name = \\'polyester\\'\\n\\n    df_aux = pd.concat([df_aux, df_polyester], axis=1)\\n    df_aux = df_aux.iloc[:, ~df_aux.columns.duplicated(keep=\\'last\\')]\\n\\n\\n    # --composition: elastomultiester\\n\\n    df_elastomultiester = df_composition.loc[df_composition[1].str.contains(\\'Elastomultiester\\', na=True),1]\\n    df_elastomultiester.name = \\'elastomultiester\\'\\n\\n    df_aux = pd.concat([df_aux, df_elastomultiester], axis=1)\\n    df_aux = df_aux.iloc[:, ~df_aux.columns.duplicated(keep=\\'last\\')]\\n\\n    # add product_id to df_aux\\n    df_aux = pd.concat([data.loc[:,\\'product_id\\'].reset_index(drop=True),df_aux], axis=1)\\n\\n\\n    #format composition \\n    df_aux[\\'cotton\\'] = df_aux[\\'cotton\\'].apply(lambda x: int(re.search(\\'\\\\d+\\',x).group(0)) /100 if pd.notnull(x) else x)\\n    df_aux[\\'polyester\\'] = df_aux[\\'polyester\\'].apply(lambda x: int(re.search(\\'\\\\d+\\',x).group(0))/100 if pd.notnull(x) else x)\\n    df_aux[\\'spandex\\'] = df_aux[\\'spandex\\'].apply(lambda x: int(re.search(\\'\\\\d+\\',x).group(0))/100 if pd.notnull(x) else x)\\n    df_aux[\\'elastomultiester\\'] = df_aux[\\'elastomultiester\\'].apply(lambda x: int(re.search(\\'\\\\d+\\',x).group(0))/100 if pd.notnull(x) else x)\\n\\n    #final join\\n    df_aux = df_aux.groupby(\\'product_id\\').max().reset_index().fillna(0)\\n    data = pd.merge(data, df_aux, on = \\'product_id\\', how = \\'left\\')\\n\\n\\n    # drop columns \\n    data = data.drop(columns = [\\'size\\', \\'composition\\'])\\n\\n    # drop duplicates\\n    data = data.drop_duplicates()\\n    \\n    # df_raw receives the clean data\\n    df_raw = data\\n    \\n    return df_raw\\n\\n\\n# Data load - load to a sqlite3 database\\n\\ndef data_load (data):\\n    \\n    # Create data_insert from df1 with adjusts at orders of columns\\n    data_insert = data[[\\n        \\'product_id\\',\\n        \\'style_ID\\', \\n        \\'color_ID\\', \\n        \\'product_name\\',\\n        \\'color_name\\',\\n        \\'fit\\',\\n        \\'product_price\\', \\n        \\'size_number\\', \\n        \\'size_model\\',\\n        \\'cotton\\', \\n        \\'spandex\\', \\n        \\'polyester\\', \\n        \\'elastomultiester\\',\\n        \\'scrapy_datetime\\' \\n    ]]\\n\\n    # Create database connection\\n    conn = create_engine(\\'sqlite:///\\'+ path + \\'db_hm.sqlite\\')\\n\\n    # Data Insert\\n    data_insert.to_sql(\\'vitrine\\', con = conn, if_exists = \\'append\\', index=False)\\n    \\n    return None\\n\\n\\n\\n\\nif __name__ == \"__main__\":\\n    \\n    path = \\'../\\'\\n    \\n    if not os.path.exists (path + \\'Logs\\'):\\n        os.mkdir(path+\\'Logs\\')\\n        \\n    logging.basicConfig(\\n        filename = path + \\'Logs/webscrapping_hm.log\\',\\n        level = logging.DEBUG, \\n        format = \\'%(asctime)s - %(levelname)s - %(name)s - %(message)s\\',\\n        datefmt = \\'%Y-%m-%d %H:%M:%S\\'\\n    )\\n    \\n    logger = logging.getLogger (\\'webscrapping_hm\\')\\n\\n    # parameters\\n    url = \\'https://www2.hm.com/en_us/men/products/jeans.html\\'\\n\\n    header = {\\'User-agent\\':\\'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/109.0.0.0 Safari/537.36\\'}\\n    \\n    # Data extract\\n    df_showcase = get_showcase (url, header)\\n    logger.info (\\'data extract done\\')\\n\\n    \\n    # Data extract by product\\n    df_all_product_details = get_all_product_details (df_showcase)\\n    logger.info (\\'data extract by product done\\')\\n    \\n    # Data tranform\\n    df_raw = data_cleaning (df_all_product_details)\\n    logger.info (\\'data transform done\\')\\n    \\n    # Data load\\n    data_load(df_raw)\\n    logger.info (\\'data load done\\')\\n'' returned non-zero exit status 1.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mCalledProcessError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[175], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mget_ipython\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_cell_magic\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpython3\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m# IMPORTS\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mimport requests\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mimport re\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mimport sqlite3\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mimport pandas as pd\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mimport seaborn as sns\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mimport logging\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mimport os\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mfrom sqlalchemy   import create_engine\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mfrom datetime     import datetime\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mfrom bs4          import BeautifulSoup\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m# Data extract - first Showcase\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mdef get_showcase(url, header):\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    \u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    # API Request - GET\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    page = requests.get(url, headers = header)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    \u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    # Beautiful Soup object\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    soup = BeautifulSoup(page.text, \u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43mhtml.parser\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    \u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    # Product Data\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    products = soup.find(\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43mul\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m, class_ = \u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43mproducts-listing small\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    product_list = products.find_all(\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43marticle\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m, class_ = \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhm-product-item\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m) \u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    # product_id\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    product_id = [p.get(\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43mdata-articlecode\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m) for p in product_list]\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    # product_category\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    product_category = [p.get(\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43mdata-category\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m) for p in product_list]\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    # product_name\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    products_list_aux = products.find_all(\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43ma\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m, class_ = \u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43mlink\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    product_name = [p.get_text(\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43mtitle\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m) for p in products_list_aux]\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    # product_price\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    product_list = products.find_all(\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43mspan\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m, class_ = \u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43mprice regular\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    product_price = [p.get_text()for p in product_list]\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    # Product Data to dataframe\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    df_showcase = pd.DataFrame([product_id, product_category, product_name, product_price]).T\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    df_showcase.columns = [\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43mproduct_id\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m,\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43mproduct_category\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m,\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43mproduct_name\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m,\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43mproduct_price\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m]\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    ## scrapy_datetime\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    df_showcase[\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43mscrapy_datetime\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m] = datetime.now().strftime( \u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m%\u001b[39;49m\u001b[38;5;124;43mY-\u001b[39;49m\u001b[38;5;124;43m%\u001b[39;49m\u001b[38;5;124;43mm-\u001b[39;49m\u001b[38;5;132;43;01m%d\u001b[39;49;00m\u001b[38;5;124;43m \u001b[39;49m\u001b[38;5;124;43m%\u001b[39;49m\u001b[38;5;124;43mH:\u001b[39;49m\u001b[38;5;124;43m%\u001b[39;49m\u001b[38;5;124;43mM:\u001b[39;49m\u001b[38;5;124;43m%\u001b[39;49m\u001b[38;5;124;43mS\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m )\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    \u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    return df_showcase\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m# Data extract by product - Each product of this showcase\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mdef get_all_product_details(data, header):\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        \u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    # empty DataFrame to receive all items \u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    df_all_product_details = pd.DataFrame()\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    # aux dataframe to assert the correct scrap\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    df_pattern = pd.DataFrame(columns=[\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43mArt. No.\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m,\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43mComposition\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m,\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43mFit\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m,\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43mSize\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m, \u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43mproduct_name\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m,\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43mproduct_price\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m])\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    # aux list to assert no different \u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    aux = []\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    for i in range(len(data)): \u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    # for each item of showcase, enter at item and collect the details!\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        logger.debug (\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43mProduct: \u001b[39;49m\u001b[38;5;132;43;01m%s\u001b[39;49;00m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m,url)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        # API Request\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        url = \u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43mhttps://www2.hm.com/en_us/productpage.\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m+ data.loc[i,\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43mproduct_id\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m]+\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m.html\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        header = \u001b[39;49m\u001b[38;5;124;43m{\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43mUser-agent\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m:\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43mMozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/109.0.0.0 Safari/537.36\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m}\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        page = requests.get(url,headers=header)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        # page.text #doctypeHTML\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        # Beautiful Soup object\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        soup = BeautifulSoup(page.text, \u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43mhtml.parser\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        # --Product List\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        product_list = soup.find_all(\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43ma\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m, class_ = \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfilter-option miniature active\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m) + soup.find_all(\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43ma\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m, class_ = \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfilter-option miniature\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m) \u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        # each miniature of color pants contains the tag with respective values, being one for active item and others tags for inactivate \u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        # color_name\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        color_name = [p.get(\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43mdata-color\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m) for p in product_list]\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        # product_id    \u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        product_id = [p.get(\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43mdata-articlecode\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m) for p in product_list]\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        # --Concat color_name and product_id to Dataframe\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        df_color = pd.DataFrame([product_id, color_name]).T\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        df_color.columns = [\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43mproduct_id\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m,\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43mcolor_name\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m]\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        for j in range(len(df_color)):\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m            # API Request\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m            url = \u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43mhttps://www2.hm.com/en_us/productpage.\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m+ df_color.loc[j,\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43mproduct_id\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m]+\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m.html\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m            header = \u001b[39;49m\u001b[38;5;124;43m{\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43mUser-agent\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m:\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43mMozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/109.0.0.0 Safari/537.36\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m}\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m            logger.debug(\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43mColor: \u001b[39;49m\u001b[38;5;132;43;01m%s\u001b[39;49;00m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m,url)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m            page = requests.get(url, headers = header)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m            # Beautiful Soup object\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m            soup = BeautifulSoup(page.text,\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43mhtml.parser\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m            # --product_name\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m            product_name = soup.find_all(\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43mh1\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m)[0]\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m            product_name = product_name.get_text()\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m            # --product_price\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m            product_price = soup.find_all(\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43mdiv\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m, class_= \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mprimary-row product-item-price\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m)[0].get_text()\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m            product_price = re.findall(r\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;124;43md+\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;124;43m.?\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;124;43md+\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m, product_price)[0]\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m            # --product_details (size_model, fit, composition, art.no)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m            product_details_list = soup.find_all(\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43mdiv\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m, class_ = \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcontent pdp-text pdp-content\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m)[0].find(\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43mdl\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m).find_all(\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43mdiv\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m            product_details = [list(filter(None,p.get_text().split(\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;124;43mn\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m))) for p in product_details_list]\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m            # product_details to DataFrame\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m            df_details = pd.DataFrame(product_details).T\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m            df_details.columns = df_details.iloc[0]\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m            # delete first row of df_details\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m            df_details = df_details.iloc[1:]\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m            # fillna in Size, Fit and Art.No with the same value,\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m            df_details = df_details.fillna(method=\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43mffill\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m            # remove Shell:, Pocket Lining:, Lining:, Pocket:\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m            df_details[\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43mComposition\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m] = df_details[\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43mComposition\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m].str.replace(\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43mShell:\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m, \u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m, regex=True)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m            df_details[\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43mComposition\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m] = df_details[\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43mComposition\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m].str.replace(\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43mPocket lining:\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m, \u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m, regex=True)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m            df_details[\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43mComposition\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m] = df_details[\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43mComposition\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m].str.replace(\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43mLining:\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m, \u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m, regex=True)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m            df_details[\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43mComposition\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m] = df_details[\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43mComposition\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m].str.replace(\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43mPocket:\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m, \u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m, regex=True)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m            # //the percentage of components was abstracted. To find the components apply df_details[\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43mComposition\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m].unique on df done\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m            # add product_price and product_name to df_details\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m            df_details[\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43mproduct_price\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m] = product_price\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m            df_details[\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43mproduct_name\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m] = product_name\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m            # garantee the same columns between product_details and a pattern, and sort the labels\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m            df_details = pd.concat([df_pattern,df_details], axis=0)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m            # //if it has some difference, a new column will appear with some Nan\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m            # rename the columns to lower case\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m            df_details.columns = df_details.columns.map(str.lower)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m            df_details = df_details.rename(columns=\u001b[39;49m\u001b[38;5;124;43m{\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43mart. no.\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m:\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43mproduct_id\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m})\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    #         df_details.columns = [\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m]\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m            # if some strange column appear, keep it and shows\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m            aux = aux + df_details.columns.to_list()\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m            if len(set(aux)) != len(df_pattern.columns):\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m                print(\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43mSome column does not fit with pattern!\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m                pass\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m            # merge df_color and df_details of one item (single product_id)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m            df_details = pd.merge(df_details, df_color, how=\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43mleft\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m, on=\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43mproduct_id\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m            # add to list of all items\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m            df_all_product_details = pd.concat([df_all_product_details, df_details])\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    # generate style_ID + color_ID\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    df_all_product_details [\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43mstyle_ID\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m] = df_all_product_details[\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43mproduct_id\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m].apply(lambda x: x[:-3])\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    df_all_product_details [\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43mcolor_ID\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m] = df_all_product_details[\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43mproduct_id\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m].apply(lambda x: x[-3:])\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    ## scrapy_datetime\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    df_all_product_details[\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43mscrapy_datetime\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m] = datetime.now().strftime( \u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m%\u001b[39;49m\u001b[38;5;124;43mY-\u001b[39;49m\u001b[38;5;124;43m%\u001b[39;49m\u001b[38;5;124;43mm-\u001b[39;49m\u001b[38;5;132;43;01m%d\u001b[39;49;00m\u001b[38;5;124;43m \u001b[39;49m\u001b[38;5;124;43m%\u001b[39;49m\u001b[38;5;124;43mH:\u001b[39;49m\u001b[38;5;124;43m%\u001b[39;49m\u001b[38;5;124;43mM:\u001b[39;49m\u001b[38;5;124;43m%\u001b[39;49m\u001b[38;5;124;43mS\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m )\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    return df_all_product_details\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m# Data transform - cleaning the data\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mdef data_cleaning (data):\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    # product_id\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    # product_name\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    data[\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43mproduct_name\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m] = data.loc[:,\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43mproduct_name\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m].apply(lambda x: x.replace(\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m \u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m,\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m_\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m).lower())\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    #product_fit\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    data[\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43mfit\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m] = data.loc[:,\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43mfit\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m].apply(lambda x: x.replace(\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m \u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m,\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m_\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m).lower())\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    #color_name\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    data[\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43mcolor_name\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m] = data.loc[:,\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43mcolor_name\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m].apply(lambda x: x.replace(\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m \u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m,\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m_\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m).lower())\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    #size_number\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    data[\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43msize_number\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m] = data[\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43msize\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m].apply(lambda x: re.search(\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;124;43md\u001b[39;49m\u001b[38;5;132;43;01m{3}\u001b[39;49;00m\u001b[38;5;124;43mcm\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m, x).group(0) if pd.notnull(x) else x)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    data[\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43msize_number\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m] = data[\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43msize_number\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m].apply(lambda x: re.search(\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;124;43md+\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m, x).group(0) if pd.notnull(x) else x)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    #size_model \u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    data[\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43msize_model\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m] = data[\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43msize\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m].str.extract(\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m(\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;124;43md+/\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;124;43md+)\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    # df1_composition outuput of break in columns of each item of \u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43mcomposition\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m separated by a comma\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    df_composition = data[\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43mcomposition\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m].str.split(\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m,\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m, expand = True).reset_index(drop=True)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    # df_aux to storage a kind of composition in a separated column \u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    df_aux = pd.DataFrame(index = np.arange(len(df1)), columns = [\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43mcotton\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m,\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43mspandex\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m,\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43mpolyester\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m,\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43melastomultiester\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m])\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    # --composition: cotton\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    df_cotton_0 = df_composition.loc[df_composition[0].str.contains(\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43mCotton\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m, na=True), 0]\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    df_cotton_0.name = \u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43mcotton\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    df_cotton_1 = df_composition.loc[df_composition[1].str.contains(\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43mCotton\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m, na=True),1]\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    df_cotton_1.name = \u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43mcotton\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    df_cotton = df_cotton_0.combine_first(df_cotton_1)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    df_aux = pd.concat ([df_aux, df_cotton],axis=1)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    df_aux = df_aux.iloc[:, ~df_aux.columns.duplicated(keep=\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43mlast\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m)]\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    # --composition: spandex\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    df_spandex_0 = df_composition.loc[df_composition[1].str.contains(\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43mSpandex\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m, na=True),1]\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    df_spandex_0.name = \u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43mspandex\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    df_spandex_1 = df_composition.loc[df_composition[2].str.contains(\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43mSpandex\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m, na=True),2]\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    df_spandex_1.name = \u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43mspandex\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    df_spandex = df1_spandex_0.combine_first(df1_spandex_1)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    df_aux = pd.concat([df_aux, df_spandex], axis=1)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    df_aux = df_aux.iloc[:, ~df_aux.columns.duplicated(keep=\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43mlast\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m)]\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    # --composition: polyester\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    df_polyester = df_composition.loc[df_composition[0].str.contains(\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43mPolyester\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m, na=True),0]\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    df_polyester.name = \u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43mpolyester\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    df_aux = pd.concat([df_aux, df_polyester], axis=1)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    df_aux = df_aux.iloc[:, ~df_aux.columns.duplicated(keep=\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43mlast\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m)]\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    # --composition: elastomultiester\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    df_elastomultiester = df_composition.loc[df_composition[1].str.contains(\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43mElastomultiester\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m, na=True),1]\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    df_elastomultiester.name = \u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43melastomultiester\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    df_aux = pd.concat([df_aux, df_elastomultiester], axis=1)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    df_aux = df_aux.iloc[:, ~df_aux.columns.duplicated(keep=\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43mlast\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m)]\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    # add product_id to df_aux\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    df_aux = pd.concat([data.loc[:,\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43mproduct_id\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m].reset_index(drop=True),df_aux], axis=1)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    #format composition \u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    df_aux[\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43mcotton\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m] = df_aux[\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43mcotton\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m].apply(lambda x: int(re.search(\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;124;43md+\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m,x).group(0)) /100 if pd.notnull(x) else x)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    df_aux[\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43mpolyester\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m] = df_aux[\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43mpolyester\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m].apply(lambda x: int(re.search(\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;124;43md+\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m,x).group(0))/100 if pd.notnull(x) else x)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    df_aux[\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43mspandex\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m] = df_aux[\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43mspandex\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m].apply(lambda x: int(re.search(\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;124;43md+\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m,x).group(0))/100 if pd.notnull(x) else x)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    df_aux[\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43melastomultiester\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m] = df_aux[\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43melastomultiester\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m].apply(lambda x: int(re.search(\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;124;43md+\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m,x).group(0))/100 if pd.notnull(x) else x)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    #final join\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    df_aux = df_aux.groupby(\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43mproduct_id\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m).max().reset_index().fillna(0)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    data = pd.merge(data, df_aux, on = \u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43mproduct_id\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m, how = \u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43mleft\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    # drop columns \u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    data = data.drop(columns = [\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43msize\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m, \u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43mcomposition\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m])\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    # drop duplicates\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    data = data.drop_duplicates()\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    \u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    # df_raw receives the clean data\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    df_raw = data\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    \u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    return df_raw\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m# Data load - load to a sqlite3 database\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mdef data_load (data):\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    \u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    # Create data_insert from df1 with adjusts at orders of columns\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    data_insert = data[[\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        \u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43mproduct_id\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m,\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        \u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43mstyle_ID\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m, \u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        \u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43mcolor_ID\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m, \u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        \u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43mproduct_name\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m,\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        \u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43mcolor_name\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m,\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        \u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43mfit\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m,\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        \u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43mproduct_price\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m, \u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        \u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43msize_number\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m, \u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        \u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43msize_model\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m,\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        \u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43mcotton\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m, \u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        \u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43mspandex\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m, \u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        \u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43mpolyester\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m, \u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        \u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43melastomultiester\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m,\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        \u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43mscrapy_datetime\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m \u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    ]]\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    # Create database connection\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    conn = create_engine(\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43msqlite:///\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m+ path + \u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43mdb_hm.sqlite\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    # Data Insert\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    data_insert.to_sql(\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43mvitrine\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m, con = conn, if_exists = \u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43mappend\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m, index=False)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    \u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    return None\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mif __name__ == \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m__main__\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m:\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    \u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    path = \u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m../\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    \u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    if not os.path.exists (path + \u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43mLogs\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m):\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        os.mkdir(path+\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43mLogs\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        \u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    logging.basicConfig(\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        filename = path + \u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43mLogs/webscrapping_hm.log\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m,\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        level = logging.DEBUG, \u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        format = \u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;132;43;01m%(asctime)s\u001b[39;49;00m\u001b[38;5;124;43m - \u001b[39;49m\u001b[38;5;132;43;01m%(levelname)s\u001b[39;49;00m\u001b[38;5;124;43m - \u001b[39;49m\u001b[38;5;132;43;01m%(name)s\u001b[39;49;00m\u001b[38;5;124;43m - \u001b[39;49m\u001b[38;5;132;43;01m%(message)s\u001b[39;49;00m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m,\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        datefmt = \u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m%\u001b[39;49m\u001b[38;5;124;43mY-\u001b[39;49m\u001b[38;5;124;43m%\u001b[39;49m\u001b[38;5;124;43mm-\u001b[39;49m\u001b[38;5;132;43;01m%d\u001b[39;49;00m\u001b[38;5;124;43m \u001b[39;49m\u001b[38;5;124;43m%\u001b[39;49m\u001b[38;5;124;43mH:\u001b[39;49m\u001b[38;5;124;43m%\u001b[39;49m\u001b[38;5;124;43mM:\u001b[39;49m\u001b[38;5;124;43m%\u001b[39;49m\u001b[38;5;124;43mS\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    )\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    \u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    logger = logging.getLogger (\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43mwebscrapping_hm\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    # parameters\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    url = \u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43mhttps://www2.hm.com/en_us/men/products/jeans.html\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    header = \u001b[39;49m\u001b[38;5;124;43m{\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43mUser-agent\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m:\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43mMozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/109.0.0.0 Safari/537.36\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m}\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    \u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    # Data extract\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    df_showcase = get_showcase (url, header)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    logger.info (\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43mdata extract done\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    \u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    # Data extract by product\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    df_all_product_details = get_all_product_details (df_showcase)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    logger.info (\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43mdata extract by product done\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    \u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    # Data tranform\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    df_raw = data_cleaning (df_all_product_details)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    logger.info (\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43mdata transform done\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    \u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    # Data load\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    data_load(df_raw)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    logger.info (\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43mdata load done\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/ds_ao_dev/lib/python3.11/site-packages/IPython/core/interactiveshell.py:2422\u001b[0m, in \u001b[0;36mInteractiveShell.run_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2420\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuiltin_trap:\n\u001b[1;32m   2421\u001b[0m     args \u001b[38;5;241m=\u001b[39m (magic_arg_s, cell)\n\u001b[0;32m-> 2422\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2423\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/miniconda3/envs/ds_ao_dev/lib/python3.11/site-packages/IPython/core/magics/script.py:153\u001b[0m, in \u001b[0;36mScriptMagics._make_script_magic.<locals>.named_script_magic\u001b[0;34m(line, cell)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    152\u001b[0m     line \u001b[38;5;241m=\u001b[39m script\n\u001b[0;32m--> 153\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshebang\u001b[49m\u001b[43m(\u001b[49m\u001b[43mline\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcell\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/ds_ao_dev/lib/python3.11/site-packages/IPython/core/magics/script.py:305\u001b[0m, in \u001b[0;36mScriptMagics.shebang\u001b[0;34m(self, line, cell)\u001b[0m\n\u001b[1;32m    300\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m args\u001b[38;5;241m.\u001b[39mraise_error \u001b[38;5;129;01mand\u001b[39;00m p\u001b[38;5;241m.\u001b[39mreturncode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    301\u001b[0m     \u001b[38;5;66;03m# If we get here and p.returncode is still None, we must have\u001b[39;00m\n\u001b[1;32m    302\u001b[0m     \u001b[38;5;66;03m# killed it but not yet seen its return code. We don't wait for it,\u001b[39;00m\n\u001b[1;32m    303\u001b[0m     \u001b[38;5;66;03m# in case it's stuck in uninterruptible sleep. -9 = SIGKILL\u001b[39;00m\n\u001b[1;32m    304\u001b[0m     rc \u001b[38;5;241m=\u001b[39m p\u001b[38;5;241m.\u001b[39mreturncode \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m9\u001b[39m\n\u001b[0;32m--> 305\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CalledProcessError(rc, cell)\n",
      "\u001b[0;31mCalledProcessError\u001b[0m: Command 'b'\\n# IMPORTS\\nimport requests\\nimport re\\nimport sqlite3\\nimport pandas as pd\\nimport seaborn as sns\\nimport logging\\nimport os\\n\\nfrom sqlalchemy   import create_engine\\nfrom datetime     import datetime\\nfrom bs4          import BeautifulSoup\\n\\n\\n# Data extract - first Showcase\\n\\ndef get_showcase(url, header):\\n    \\n    # API Request - GET\\n    page = requests.get(url, headers = header)\\n    \\n    # Beautiful Soup object\\n    soup = BeautifulSoup(page.text, \\'html.parser\\')\\n    \\n    # Product Data\\n    products = soup.find(\\'ul\\', class_ = \\'products-listing small\\')\\n    product_list = products.find_all(\\'article\\', class_ = \"hm-product-item\") \\n\\n    # product_id\\n    product_id = [p.get(\\'data-articlecode\\') for p in product_list]\\n\\n    # product_category\\n    product_category = [p.get(\\'data-category\\') for p in product_list]\\n\\n    # product_name\\n    products_list_aux = products.find_all(\\'a\\', class_ = \\'link\\')\\n    product_name = [p.get_text(\\'title\\') for p in products_list_aux]\\n\\n    # product_price\\n    product_list = products.find_all(\\'span\\', class_ = \\'price regular\\')\\n    product_price = [p.get_text()for p in product_list]\\n\\n\\n    # Product Data to dataframe\\n    df_showcase = pd.DataFrame([product_id, product_category, product_name, product_price]).T\\n    df_showcase.columns = [\\'product_id\\',\\'product_category\\',\\'product_name\\',\\'product_price\\']\\n\\n    ## scrapy_datetime\\n    df_showcase[\\'scrapy_datetime\\'] = datetime.now().strftime( \\'%Y-%m-%d %H:%M:%S\\' )\\n    \\n    return df_showcase\\n\\n\\n\\n# Data extract by product - Each product of this showcase\\n\\ndef get_all_product_details(data, header):\\n        \\n    # empty DataFrame to receive all items \\n    df_all_product_details = pd.DataFrame()\\n\\n    # aux dataframe to assert the correct scrap\\n    df_pattern = pd.DataFrame(columns=[\\'Art. No.\\',\\'Composition\\',\\'Fit\\',\\'Size\\', \\'product_name\\',\\'product_price\\'])\\n\\n    # aux list to assert no different \\n    aux = []\\n\\n    for i in range(len(data)): \\n    # for each item of showcase, enter at item and collect the details!\\n        logger.debug (\\'Product: %s\\',url)\\n\\n        # API Request\\n        url = \\'https://www2.hm.com/en_us/productpage.\\'+ data.loc[i,\\'product_id\\']+\\'.html\\'\\n\\n        header = {\\'User-agent\\':\\'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/109.0.0.0 Safari/537.36\\'}\\n\\n        page = requests.get(url,headers=header)\\n\\n        # page.text #doctypeHTML\\n\\n\\n        # Beautiful Soup object\\n        soup = BeautifulSoup(page.text, \\'html.parser\\')\\n\\n\\n        # --Product List\\n        product_list = soup.find_all(\\'a\\', class_ = \"filter-option miniature active\") + soup.find_all(\\'a\\', class_ = \"filter-option miniature\") \\n        # each miniature of color pants contains the tag with respective values, being one for active item and others tags for inactivate \\n\\n        # color_name\\n        color_name = [p.get(\\'data-color\\') for p in product_list]\\n\\n        # product_id    \\n        product_id = [p.get(\\'data-articlecode\\') for p in product_list]\\n\\n        # --Concat color_name and product_id to Dataframe\\n        df_color = pd.DataFrame([product_id, color_name]).T\\n        df_color.columns = [\\'product_id\\',\\'color_name\\']\\n\\n\\n        for j in range(len(df_color)):\\n\\n            # API Request\\n            url = \\'https://www2.hm.com/en_us/productpage.\\'+ df_color.loc[j,\\'product_id\\']+\\'.html\\'\\n            header = {\\'User-agent\\':\\'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/109.0.0.0 Safari/537.36\\'}\\n            logger.debug(\\'Color: %s\\',url)\\n\\n            page = requests.get(url, headers = header)\\n\\n            # Beautiful Soup object\\n            soup = BeautifulSoup(page.text,\\'html.parser\\')\\n\\n            # --product_name\\n            product_name = soup.find_all(\\'h1\\')[0]\\n            product_name = product_name.get_text()\\n\\n            # --product_price\\n            product_price = soup.find_all(\\'div\\', class_= \"primary-row product-item-price\")[0].get_text()\\n            product_price = re.findall(r\\'\\\\d+\\\\.?\\\\d+\\', product_price)[0]\\n\\n            # --product_details (size_model, fit, composition, art.no)\\n            product_details_list = soup.find_all(\\'div\\', class_ = \"content pdp-text pdp-content\")[0].find(\\'dl\\').find_all(\\'div\\')\\n            product_details = [list(filter(None,p.get_text().split(\\'\\\\n\\'))) for p in product_details_list]\\n\\n            # product_details to DataFrame\\n            df_details = pd.DataFrame(product_details).T\\n            df_details.columns = df_details.iloc[0]\\n\\n            # delete first row of df_details\\n            df_details = df_details.iloc[1:]\\n\\n            # fillna in Size, Fit and Art.No with the same value,\\n            df_details = df_details.fillna(method=\\'ffill\\')\\n\\n            # remove Shell:, Pocket Lining:, Lining:, Pocket:\\n            df_details[\\'Composition\\'] = df_details[\\'Composition\\'].str.replace(\\'Shell:\\', \\'\\', regex=True)\\n            df_details[\\'Composition\\'] = df_details[\\'Composition\\'].str.replace(\\'Pocket lining:\\', \\'\\', regex=True)\\n            df_details[\\'Composition\\'] = df_details[\\'Composition\\'].str.replace(\\'Lining:\\', \\'\\', regex=True)\\n            df_details[\\'Composition\\'] = df_details[\\'Composition\\'].str.replace(\\'Pocket:\\', \\'\\', regex=True)\\n            # //the percentage of components was abstracted. To find the components apply df_details[\\'Composition\\'].unique on df done\\n\\n            # add product_price and product_name to df_details\\n            df_details[\\'product_price\\'] = product_price\\n            df_details[\\'product_name\\'] = product_name\\n\\n\\n            # garantee the same columns between product_details and a pattern, and sort the labels\\n            df_details = pd.concat([df_pattern,df_details], axis=0)\\n            # //if it has some difference, a new column will appear with some Nan\\n\\n            # rename the columns to lower case\\n            df_details.columns = df_details.columns.map(str.lower)\\n            df_details = df_details.rename(columns={\\'art. no.\\':\\'product_id\\'})\\n    #         df_details.columns = [\\'\\']\\n\\n\\n            # if some strange column appear, keep it and shows\\n            aux = aux + df_details.columns.to_list()\\n            if len(set(aux)) != len(df_pattern.columns):\\n                print(\\'Some column does not fit with pattern!\\')\\n                pass\\n\\n            # merge df_color and df_details of one item (single product_id)\\n            df_details = pd.merge(df_details, df_color, how=\\'left\\', on=\\'product_id\\')\\n\\n            # add to list of all items\\n            df_all_product_details = pd.concat([df_all_product_details, df_details])\\n\\n\\n    # generate style_ID + color_ID\\n    df_all_product_details [\\'style_ID\\'] = df_all_product_details[\\'product_id\\'].apply(lambda x: x[:-3])\\n    df_all_product_details [\\'color_ID\\'] = df_all_product_details[\\'product_id\\'].apply(lambda x: x[-3:])\\n\\n    ## scrapy_datetime\\n    df_all_product_details[\\'scrapy_datetime\\'] = datetime.now().strftime( \\'%Y-%m-%d %H:%M:%S\\' )\\n\\n    return df_all_product_details\\n\\n\\n\\n# Data transform - cleaning the data\\n\\ndef data_cleaning (data):\\n\\n    # product_id\\n\\n    # product_name\\n    data[\\'product_name\\'] = data.loc[:,\\'product_name\\'].apply(lambda x: x.replace(\\' \\',\\'_\\').lower())\\n\\n    #product_fit\\n    data[\\'fit\\'] = data.loc[:,\\'fit\\'].apply(lambda x: x.replace(\\' \\',\\'_\\').lower())\\n\\n    #color_name\\n    data[\\'color_name\\'] = data.loc[:,\\'color_name\\'].apply(lambda x: x.replace(\\' \\',\\'_\\').lower())\\n\\n    #size_number\\n    data[\\'size_number\\'] = data[\\'size\\'].apply(lambda x: re.search(\\'\\\\d{3}cm\\', x).group(0) if pd.notnull(x) else x)\\n    data[\\'size_number\\'] = data[\\'size_number\\'].apply(lambda x: re.search(\\'\\\\d+\\', x).group(0) if pd.notnull(x) else x)\\n\\n    #size_model \\n    data[\\'size_model\\'] = data[\\'size\\'].str.extract(\\'(\\\\d+/\\\\d+)\\')\\n\\n\\n\\n    # df1_composition outuput of break in columns of each item of \\'composition\\' separated by a comma\\n    df_composition = data[\\'composition\\'].str.split(\\',\\', expand = True).reset_index(drop=True)\\n\\n    # df_aux to storage a kind of composition in a separated column \\n    df_aux = pd.DataFrame(index = np.arange(len(df1)), columns = [\\'cotton\\',\\'spandex\\',\\'polyester\\',\\'elastomultiester\\'])\\n\\n\\n\\n    # --composition: cotton\\n    df_cotton_0 = df_composition.loc[df_composition[0].str.contains(\\'Cotton\\', na=True), 0]\\n    df_cotton_0.name = \\'cotton\\'\\n    df_cotton_1 = df_composition.loc[df_composition[1].str.contains(\\'Cotton\\', na=True),1]\\n    df_cotton_1.name = \\'cotton\\'\\n\\n    df_cotton = df_cotton_0.combine_first(df_cotton_1)\\n\\n    df_aux = pd.concat ([df_aux, df_cotton],axis=1)\\n    df_aux = df_aux.iloc[:, ~df_aux.columns.duplicated(keep=\\'last\\')]\\n\\n    # --composition: spandex\\n\\n    df_spandex_0 = df_composition.loc[df_composition[1].str.contains(\\'Spandex\\', na=True),1]\\n    df_spandex_0.name = \\'spandex\\'\\n    df_spandex_1 = df_composition.loc[df_composition[2].str.contains(\\'Spandex\\', na=True),2]\\n    df_spandex_1.name = \\'spandex\\'\\n\\n    df_spandex = df1_spandex_0.combine_first(df1_spandex_1)\\n\\n    df_aux = pd.concat([df_aux, df_spandex], axis=1)\\n    df_aux = df_aux.iloc[:, ~df_aux.columns.duplicated(keep=\\'last\\')]\\n\\n    # --composition: polyester\\n\\n    df_polyester = df_composition.loc[df_composition[0].str.contains(\\'Polyester\\', na=True),0]\\n    df_polyester.name = \\'polyester\\'\\n\\n    df_aux = pd.concat([df_aux, df_polyester], axis=1)\\n    df_aux = df_aux.iloc[:, ~df_aux.columns.duplicated(keep=\\'last\\')]\\n\\n\\n    # --composition: elastomultiester\\n\\n    df_elastomultiester = df_composition.loc[df_composition[1].str.contains(\\'Elastomultiester\\', na=True),1]\\n    df_elastomultiester.name = \\'elastomultiester\\'\\n\\n    df_aux = pd.concat([df_aux, df_elastomultiester], axis=1)\\n    df_aux = df_aux.iloc[:, ~df_aux.columns.duplicated(keep=\\'last\\')]\\n\\n    # add product_id to df_aux\\n    df_aux = pd.concat([data.loc[:,\\'product_id\\'].reset_index(drop=True),df_aux], axis=1)\\n\\n\\n    #format composition \\n    df_aux[\\'cotton\\'] = df_aux[\\'cotton\\'].apply(lambda x: int(re.search(\\'\\\\d+\\',x).group(0)) /100 if pd.notnull(x) else x)\\n    df_aux[\\'polyester\\'] = df_aux[\\'polyester\\'].apply(lambda x: int(re.search(\\'\\\\d+\\',x).group(0))/100 if pd.notnull(x) else x)\\n    df_aux[\\'spandex\\'] = df_aux[\\'spandex\\'].apply(lambda x: int(re.search(\\'\\\\d+\\',x).group(0))/100 if pd.notnull(x) else x)\\n    df_aux[\\'elastomultiester\\'] = df_aux[\\'elastomultiester\\'].apply(lambda x: int(re.search(\\'\\\\d+\\',x).group(0))/100 if pd.notnull(x) else x)\\n\\n    #final join\\n    df_aux = df_aux.groupby(\\'product_id\\').max().reset_index().fillna(0)\\n    data = pd.merge(data, df_aux, on = \\'product_id\\', how = \\'left\\')\\n\\n\\n    # drop columns \\n    data = data.drop(columns = [\\'size\\', \\'composition\\'])\\n\\n    # drop duplicates\\n    data = data.drop_duplicates()\\n    \\n    # df_raw receives the clean data\\n    df_raw = data\\n    \\n    return df_raw\\n\\n\\n# Data load - load to a sqlite3 database\\n\\ndef data_load (data):\\n    \\n    # Create data_insert from df1 with adjusts at orders of columns\\n    data_insert = data[[\\n        \\'product_id\\',\\n        \\'style_ID\\', \\n        \\'color_ID\\', \\n        \\'product_name\\',\\n        \\'color_name\\',\\n        \\'fit\\',\\n        \\'product_price\\', \\n        \\'size_number\\', \\n        \\'size_model\\',\\n        \\'cotton\\', \\n        \\'spandex\\', \\n        \\'polyester\\', \\n        \\'elastomultiester\\',\\n        \\'scrapy_datetime\\' \\n    ]]\\n\\n    # Create database connection\\n    conn = create_engine(\\'sqlite:///\\'+ path + \\'db_hm.sqlite\\')\\n\\n    # Data Insert\\n    data_insert.to_sql(\\'vitrine\\', con = conn, if_exists = \\'append\\', index=False)\\n    \\n    return None\\n\\n\\n\\n\\nif __name__ == \"__main__\":\\n    \\n    path = \\'../\\'\\n    \\n    if not os.path.exists (path + \\'Logs\\'):\\n        os.mkdir(path+\\'Logs\\')\\n        \\n    logging.basicConfig(\\n        filename = path + \\'Logs/webscrapping_hm.log\\',\\n        level = logging.DEBUG, \\n        format = \\'%(asctime)s - %(levelname)s - %(name)s - %(message)s\\',\\n        datefmt = \\'%Y-%m-%d %H:%M:%S\\'\\n    )\\n    \\n    logger = logging.getLogger (\\'webscrapping_hm\\')\\n\\n    # parameters\\n    url = \\'https://www2.hm.com/en_us/men/products/jeans.html\\'\\n\\n    header = {\\'User-agent\\':\\'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/109.0.0.0 Safari/537.36\\'}\\n    \\n    # Data extract\\n    df_showcase = get_showcase (url, header)\\n    logger.info (\\'data extract done\\')\\n\\n    \\n    # Data extract by product\\n    df_all_product_details = get_all_product_details (df_showcase)\\n    logger.info (\\'data extract by product done\\')\\n    \\n    # Data tranform\\n    df_raw = data_cleaning (df_all_product_details)\\n    logger.info (\\'data transform done\\')\\n    \\n    # Data load\\n    data_load(df_raw)\\n    logger.info (\\'data load done\\')\\n'' returned non-zero exit status 1."
     ]
    }
   ],
   "source": [
    "# IMPORTS\n",
    "import requests\n",
    "import re\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import logging\n",
    "import os\n",
    "\n",
    "from sqlalchemy   import create_engine\n",
    "from datetime     import datetime\n",
    "from bs4          import BeautifulSoup\n",
    "\n",
    "\n",
    "# Data extract - first Showcase\n",
    "\n",
    "def get_showcase(url, header):\n",
    "    \n",
    "    # API Request - GET\n",
    "    page = requests.get(url, headers = header)\n",
    "    \n",
    "    # Beautiful Soup object\n",
    "    soup = BeautifulSoup(page.text, 'html.parser')\n",
    "    \n",
    "    # Product Data\n",
    "    products = soup.find('ul', class_ = 'products-listing small')\n",
    "    product_list = products.find_all('article', class_ = \"hm-product-item\") \n",
    "\n",
    "    # product_id\n",
    "    product_id = [p.get('data-articlecode') for p in product_list]\n",
    "\n",
    "    # product_category\n",
    "    product_category = [p.get('data-category') for p in product_list]\n",
    "\n",
    "    # product_name\n",
    "    products_list_aux = products.find_all('a', class_ = 'link')\n",
    "    product_name = [p.get_text('title') for p in products_list_aux]\n",
    "\n",
    "    # product_price\n",
    "    product_list = products.find_all('span', class_ = 'price regular')\n",
    "    product_price = [p.get_text()for p in product_list]\n",
    "\n",
    "\n",
    "    # Product Data to dataframe\n",
    "    df_showcase = pd.DataFrame([product_id, product_category, product_name, product_price]).T\n",
    "    df_showcase.columns = ['product_id','product_category','product_name','product_price']\n",
    "\n",
    "    ## scrapy_datetime\n",
    "    df_showcase['scrapy_datetime'] = datetime.now().strftime( '%Y-%m-%d %H:%M:%S' )\n",
    "    \n",
    "    return df_showcase\n",
    "\n",
    "\n",
    "\n",
    "# Data extract by product - Each product of this showcase\n",
    "\n",
    "def get_all_product_details(data, header):\n",
    "        \n",
    "    # empty DataFrame to receive all items \n",
    "    df_all_product_details = pd.DataFrame()\n",
    "\n",
    "    # aux dataframe to assert the correct scrap\n",
    "    df_pattern = pd.DataFrame(columns=['Art. No.','Composition','Fit','Size', 'product_name','product_price'])\n",
    "\n",
    "    # aux list to assert no different \n",
    "    aux = []\n",
    "\n",
    "    for i in range(len(data)): \n",
    "    # for each item of showcase, enter at item and collect the details!\n",
    "        logger.debug ('Product: %s',url)\n",
    "\n",
    "        # API Request\n",
    "        url = 'https://www2.hm.com/en_us/productpage.'+ data.loc[i,'product_id']+'.html'\n",
    "\n",
    "        header = {'User-agent':'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/109.0.0.0 Safari/537.36'}\n",
    "\n",
    "        page = requests.get(url,headers=header)\n",
    "\n",
    "        # page.text #doctypeHTML\n",
    "\n",
    "\n",
    "        # Beautiful Soup object\n",
    "        soup = BeautifulSoup(page.text, 'html.parser')\n",
    "\n",
    "\n",
    "        # --Product List\n",
    "        product_list = soup.find_all('a', class_ = \"filter-option miniature active\") + soup.find_all('a', class_ = \"filter-option miniature\") \n",
    "        # each miniature of color pants contains the tag with respective values, being one for active item and others tags for inactivate \n",
    "\n",
    "        # color_name\n",
    "        color_name = [p.get('data-color') for p in product_list]\n",
    "\n",
    "        # product_id    \n",
    "        product_id = [p.get('data-articlecode') for p in product_list]\n",
    "\n",
    "        # --Concat color_name and product_id to Dataframe\n",
    "        df_color = pd.DataFrame([product_id, color_name]).T\n",
    "        df_color.columns = ['product_id','color_name']\n",
    "\n",
    "\n",
    "        for j in range(len(df_color)):\n",
    "\n",
    "            # API Request\n",
    "            url = 'https://www2.hm.com/en_us/productpage.'+ df_color.loc[j,'product_id']+'.html'\n",
    "            header = {'User-agent':'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/109.0.0.0 Safari/537.36'}\n",
    "            logger.debug('Color: %s',url)\n",
    "\n",
    "            page = requests.get(url, headers = header)\n",
    "\n",
    "            # Beautiful Soup object\n",
    "            soup = BeautifulSoup(page.text,'html.parser')\n",
    "\n",
    "            # --product_name\n",
    "            product_name = soup.find_all('h1')[0]\n",
    "            product_name = product_name.get_text()\n",
    "\n",
    "            # --product_price\n",
    "            product_price = soup.find_all('div', class_= \"primary-row product-item-price\")[0].get_text()\n",
    "            product_price = re.findall(r'\\d+\\.?\\d+', product_price)[0]\n",
    "\n",
    "            # --product_details (size_model, fit, composition, art.no)\n",
    "            product_details_list = soup.find_all('div', class_ = \"content pdp-text pdp-content\")[0].find('dl').find_all('div')\n",
    "            product_details = [list(filter(None,p.get_text().split('\\n'))) for p in product_details_list]\n",
    "\n",
    "            # product_details to DataFrame\n",
    "            df_details = pd.DataFrame(product_details).T\n",
    "            df_details.columns = df_details.iloc[0]\n",
    "\n",
    "            # delete first row of df_details\n",
    "            df_details = df_details.iloc[1:]\n",
    "\n",
    "            # fillna in Size, Fit and Art.No with the same value,\n",
    "            df_details = df_details.fillna(method='ffill')\n",
    "\n",
    "            # remove Shell:, Pocket Lining:, Lining:, Pocket:\n",
    "            df_details['Composition'] = df_details['Composition'].str.replace('Shell:', '', regex=True)\n",
    "            df_details['Composition'] = df_details['Composition'].str.replace('Pocket lining:', '', regex=True)\n",
    "            df_details['Composition'] = df_details['Composition'].str.replace('Lining:', '', regex=True)\n",
    "            df_details['Composition'] = df_details['Composition'].str.replace('Pocket:', '', regex=True)\n",
    "            # //the percentage of components was abstracted. To find the components apply df_details['Composition'].unique on df done\n",
    "\n",
    "            # add product_price and product_name to df_details\n",
    "            df_details['product_price'] = product_price\n",
    "            df_details['product_name'] = product_name\n",
    "\n",
    "\n",
    "            # garantee the same columns between product_details and a pattern, and sort the labels\n",
    "            df_details = pd.concat([df_pattern,df_details], axis=0)\n",
    "            # //if it has some difference, a new column will appear with some Nan\n",
    "\n",
    "            # rename the columns to lower case\n",
    "            df_details.columns = df_details.columns.map(str.lower)\n",
    "            df_details = df_details.rename(columns={'art. no.':'product_id'})\n",
    "    #         df_details.columns = ['']\n",
    "\n",
    "\n",
    "            # if some strange column appear, keep it and shows\n",
    "            aux = aux + df_details.columns.to_list()\n",
    "            if len(set(aux)) != len(df_pattern.columns):\n",
    "                print('Some column does not fit with pattern!')\n",
    "                pass\n",
    "\n",
    "            # merge df_color and df_details of one item (single product_id)\n",
    "            df_details = pd.merge(df_details, df_color, how='left', on='product_id')\n",
    "\n",
    "            # add to list of all items\n",
    "            df_all_product_details = pd.concat([df_all_product_details, df_details])\n",
    "\n",
    "\n",
    "    # generate style_ID + color_ID\n",
    "    df_all_product_details ['style_ID'] = df_all_product_details['product_id'].apply(lambda x: x[:-3])\n",
    "    df_all_product_details ['color_ID'] = df_all_product_details['product_id'].apply(lambda x: x[-3:])\n",
    "\n",
    "    ## scrapy_datetime\n",
    "    df_all_product_details['scrapy_datetime'] = datetime.now().strftime( '%Y-%m-%d %H:%M:%S' )\n",
    "\n",
    "    return df_all_product_details\n",
    "\n",
    "\n",
    "\n",
    "# Data transform - cleaning the data\n",
    "\n",
    "def data_cleaning (data):\n",
    "\n",
    "    # product_id\n",
    "\n",
    "    # product_name\n",
    "    data['product_name'] = data.loc[:,'product_name'].apply(lambda x: x.replace(' ','_').lower())\n",
    "\n",
    "    #product_fit\n",
    "    data['fit'] = data.loc[:,'fit'].apply(lambda x: x.replace(' ','_').lower())\n",
    "\n",
    "    #color_name\n",
    "    data['color_name'] = data.loc[:,'color_name'].apply(lambda x: x.replace(' ','_').lower())\n",
    "\n",
    "    #size_number\n",
    "    data['size_number'] = data['size'].apply(lambda x: re.search('\\d{3}cm', x).group(0) if pd.notnull(x) else x)\n",
    "    data['size_number'] = data['size_number'].apply(lambda x: re.search('\\d+', x).group(0) if pd.notnull(x) else x)\n",
    "\n",
    "    #size_model \n",
    "    data['size_model'] = data['size'].str.extract('(\\d+/\\d+)')\n",
    "\n",
    "\n",
    "\n",
    "    # df1_composition outuput of break in columns of each item of 'composition' separated by a comma\n",
    "    df_composition = data['composition'].str.split(',', expand = True).reset_index(drop=True)\n",
    "\n",
    "    # df_aux to storage a kind of composition in a separated column \n",
    "    df_aux = pd.DataFrame(index = np.arange(len(df1)), columns = ['cotton','spandex','polyester','elastomultiester'])\n",
    "\n",
    "\n",
    "\n",
    "    # --composition: cotton\n",
    "    df_cotton_0 = df_composition.loc[df_composition[0].str.contains('Cotton', na=True), 0]\n",
    "    df_cotton_0.name = 'cotton'\n",
    "    df_cotton_1 = df_composition.loc[df_composition[1].str.contains('Cotton', na=True),1]\n",
    "    df_cotton_1.name = 'cotton'\n",
    "\n",
    "    df_cotton = df_cotton_0.combine_first(df_cotton_1)\n",
    "\n",
    "    df_aux = pd.concat ([df_aux, df_cotton],axis=1)\n",
    "    df_aux = df_aux.iloc[:, ~df_aux.columns.duplicated(keep='last')]\n",
    "\n",
    "    # --composition: spandex\n",
    "\n",
    "    df_spandex_0 = df_composition.loc[df_composition[1].str.contains('Spandex', na=True),1]\n",
    "    df_spandex_0.name = 'spandex'\n",
    "    df_spandex_1 = df_composition.loc[df_composition[2].str.contains('Spandex', na=True),2]\n",
    "    df_spandex_1.name = 'spandex'\n",
    "\n",
    "    df_spandex = df1_spandex_0.combine_first(df1_spandex_1)\n",
    "\n",
    "    df_aux = pd.concat([df_aux, df_spandex], axis=1)\n",
    "    df_aux = df_aux.iloc[:, ~df_aux.columns.duplicated(keep='last')]\n",
    "\n",
    "    # --composition: polyester\n",
    "\n",
    "    df_polyester = df_composition.loc[df_composition[0].str.contains('Polyester', na=True),0]\n",
    "    df_polyester.name = 'polyester'\n",
    "\n",
    "    df_aux = pd.concat([df_aux, df_polyester], axis=1)\n",
    "    df_aux = df_aux.iloc[:, ~df_aux.columns.duplicated(keep='last')]\n",
    "\n",
    "\n",
    "    # --composition: elastomultiester\n",
    "\n",
    "    df_elastomultiester = df_composition.loc[df_composition[1].str.contains('Elastomultiester', na=True),1]\n",
    "    df_elastomultiester.name = 'elastomultiester'\n",
    "\n",
    "    df_aux = pd.concat([df_aux, df_elastomultiester], axis=1)\n",
    "    df_aux = df_aux.iloc[:, ~df_aux.columns.duplicated(keep='last')]\n",
    "\n",
    "    # add product_id to df_aux\n",
    "    df_aux = pd.concat([data.loc[:,'product_id'].reset_index(drop=True),df_aux], axis=1)\n",
    "\n",
    "\n",
    "    #format composition \n",
    "    df_aux['cotton'] = df_aux['cotton'].apply(lambda x: int(re.search('\\d+',x).group(0)) /100 if pd.notnull(x) else x)\n",
    "    df_aux['polyester'] = df_aux['polyester'].apply(lambda x: int(re.search('\\d+',x).group(0))/100 if pd.notnull(x) else x)\n",
    "    df_aux['spandex'] = df_aux['spandex'].apply(lambda x: int(re.search('\\d+',x).group(0))/100 if pd.notnull(x) else x)\n",
    "    df_aux['elastomultiester'] = df_aux['elastomultiester'].apply(lambda x: int(re.search('\\d+',x).group(0))/100 if pd.notnull(x) else x)\n",
    "\n",
    "    #final join\n",
    "    df_aux = df_aux.groupby('product_id').max().reset_index().fillna(0)\n",
    "    data = pd.merge(data, df_aux, on = 'product_id', how = 'left')\n",
    "\n",
    "\n",
    "    # drop columns \n",
    "    data = data.drop(columns = ['size', 'composition'])\n",
    "\n",
    "    # drop duplicates\n",
    "    data = data.drop_duplicates()\n",
    "    \n",
    "    # df_raw receives the clean data\n",
    "    df_raw = data\n",
    "    \n",
    "    return df_raw\n",
    "\n",
    "\n",
    "# Data load - load to a sqlite3 database\n",
    "\n",
    "def data_load (data, path):\n",
    "    \n",
    "    # Create data_insert from df1 with adjusts at orders of columns\n",
    "    data_insert = data[[\n",
    "        'product_id',\n",
    "        'style_ID', \n",
    "        'color_ID', \n",
    "        'product_name',\n",
    "        'color_name',\n",
    "        'fit',\n",
    "        'product_price', \n",
    "        'size_number', \n",
    "        'size_model',\n",
    "        'cotton', \n",
    "        'spandex', \n",
    "        'polyester', \n",
    "        'elastomultiester',\n",
    "        'scrapy_datetime' \n",
    "    ]]\n",
    "\n",
    "    # Create database connection\n",
    "    conn = create_engine('sqlite:///'+ path + 'db_hm.sqlite')\n",
    "\n",
    "    # Data Insert\n",
    "    data_insert.to_sql('vitrine', con = conn, if_exists = 'append', index=False)\n",
    "    \n",
    "    return None\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    path = '../../'\n",
    "    \n",
    "    if not os.path.exists (path + 'Logs'):\n",
    "        os.mkdir(path+'Logs')\n",
    "        \n",
    "    logging.basicConfig(\n",
    "        filename = path + 'Logs/webscrapping_hm.log',\n",
    "        level = logging.DEBUG, \n",
    "        format = '%(asctime)s - %(levelname)s - %(name)s - %(message)s',\n",
    "        datefmt = '%Y-%m-%d %H:%M:%S'\n",
    "    )\n",
    "    \n",
    "    logger = logging.getLogger ('webscrapping_hm')\n",
    "\n",
    "    # parameters\n",
    "    url = 'https://www2.hm.com/en_us/men/products/jeans.html'\n",
    "\n",
    "    header = {'User-agent':'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/109.0.0.0 Safari/537.36'}\n",
    "    \n",
    "    # Data extract\n",
    "    df_showcase = get_showcase (url, header)\n",
    "    logger.info ('data extract done')\n",
    "\n",
    "    \n",
    "    # Data extract by product\n",
    "    df_all_product_details = get_all_product_details (df_showcase, header)\n",
    "    logger.info ('data extract by product done')\n",
    "    \n",
    "    # Data tranform\n",
    "    df_raw = data_cleaning (df_all_product_details)\n",
    "    logger.info ('data transform done')\n",
    "    \n",
    "    # Data load\n",
    "    data_load(df_raw, path)\n",
    "    logger.info ('data load done')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88cd0891",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fea4dc93",
   "metadata": {},
   "source": [
    "## Query tester"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e973775",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "145.087px",
    "left": "1146.76px",
    "right": "20px",
    "top": "122px",
    "width": "350px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
